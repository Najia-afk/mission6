{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mission 6: Feasibility Study of Product Classification Engine\n",
    "\n",
    "## 1. Introduction\n",
    "**Objective**: Evaluate the feasibility of automatic product classification using text descriptions and images for an e-commerce marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "### 2.1 Components\n",
    "| Modality | Description | Source | Notes |\n",
    "|----------|-------------|--------|-------|\n",
    "| Images | Product photos (RGB) | Flipkart dataset | Variable resolutions; resized to 224Ã—224 |\n",
    "| Text | Product titles / descriptions (English) | Metadata CSV | Cleaned: lowercased, punctuation stripped, stopwords partially removed |\n",
    "| Labels | Product category identifiers | Metadata CSV | Multi-class (N classes) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Plotly to properly render in HTML exports\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set the renderer for notebook display\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Configure global theme for consistent appearance\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import os\n",
    "# Set environment variable to disable oneDNN optimizations to avoid numerical differences\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Import tqdm for progress bars\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Read all CSV files from dataset/Flipkart directory with glob\n",
    "csv_files = glob.glob('dataset/Flipkart/flipkart*.csv')\n",
    "\n",
    "# Import the CSV files into a dataframe\n",
    "df = pd.read_csv(csv_files[0])\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2.2 Basic Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.analyze_value_specifications import SpecificationsValueAnalyzer\n",
    "\n",
    "analyzer = SpecificationsValueAnalyzer(df)\n",
    "value_analysis = analyzer.get_top_values(top_keys=5, top_values=5)\n",
    "value_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Class Balance (Post-Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radial icicle chart to visualize the top values\n",
    "fig = analyzer.create_radial_icicle_chart(top_keys=10, top_values=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.analyze_category_tree import CategoryTreeAnalyzer\n",
    "\n",
    "# Create analyzer instance with your dataframe\n",
    "category_analyzer = CategoryTreeAnalyzer(df)\n",
    "\n",
    "# Create and display the radial category chart\n",
    "fig = category_analyzer.create_radial_category_chart(max_depth=9)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Basic NLP Classification Feasibility Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 3.1 Text Preprocessing\n",
    "**Steps**:\n",
    "- Clean text data\n",
    "- Remove stopwords\n",
    "- Perform stemming/lemmatization\n",
    "- Handle special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TextPreprocessor class\n",
    "from src.classes.preprocess_text import TextPreprocessor\n",
    "\n",
    "# Create processor instance\n",
    "processor = TextPreprocessor()\n",
    "\n",
    "# 1. Demonstrate functions with a clear example sentence\n",
    "print(\"ðŸ” TEXT PREPROCESSING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_sentence = \"To be or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and, by opposing, end them?\"\n",
    "\n",
    "print(f\"Original: '{test_sentence}'\")\n",
    "print(f\"Tokenized: {processor.tokenize_sentence(test_sentence)}\")\n",
    "print(f\"Stemmed: '{processor.stem_sentence(test_sentence)}'\")\n",
    "print(f\"Lemmatized: '{processor.lemmatize_sentence(test_sentence)}'\")\n",
    "print(f\"Fully preprocessed: '{processor.preprocess(test_sentence)}'\")\n",
    "\n",
    "# 2. Process the DataFrame columns efficiently\n",
    "print(\"\\nðŸ”„ APPLYING TO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply preprocessing to product names\n",
    "df['product_name_lemmatized'] = df['product_name'].apply(processor.preprocess)\n",
    "df['product_name_stemmed'] = df['product_name'].apply(processor.stem_text)\n",
    "df['product_category'] = df['product_category_tree'].apply(processor.extract_top_category)\n",
    "\n",
    "# 3. Show a few examples of the transformations\n",
    "print(\"\\nðŸ“‹ TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "comparison_data = []\n",
    "\n",
    "for i in range(min(5, len(df))):\n",
    "    original = df['product_name'].iloc[i]\n",
    "    lemmatized = df['product_name_lemmatized'].iloc[i]\n",
    "    stemmed = df['product_name_stemmed'].iloc[i]\n",
    "    \n",
    "    # Truncate long examples for display\n",
    "    max_len = 50\n",
    "    orig_display = original[:max_len] + ('...' if len(original) > max_len else '')\n",
    "    lem_display = lemmatized[:max_len] + ('...' if len(lemmatized) > max_len else '')\n",
    "    stem_display = stemmed[:max_len] + ('...' if len(stemmed) > max_len else '')\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Original': orig_display,\n",
    "        'Lemmatized': lem_display,\n",
    "        'Stemmed': stem_display\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\nðŸ“Š PREPROCESSING STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "total_words_before = df['product_name'].str.split().str.len().sum()\n",
    "total_words_lemmatized = df['product_name_lemmatized'].str.split().str.len().sum()\n",
    "total_words_stemmed = df['product_name_stemmed'].str.split().str.len().sum()\n",
    "\n",
    "lem_reduction = ((total_words_before - total_words_lemmatized) / total_words_before) * 100\n",
    "stem_reduction = ((total_words_before - total_words_stemmed) / total_words_before) * 100\n",
    "\n",
    "print(f\"Total words before processing: {total_words_before:,}\")\n",
    "print(f\"Words after lemmatization: {total_words_lemmatized:,} ({lem_reduction:.1f}% reduction)\")\n",
    "print(f\"Words after stemming: {total_words_stemmed:,} ({stem_reduction:.1f}% reduction)\")\n",
    "print(f\"Unique categories extracted: {df['product_category'].nunique()}\")\n",
    "\n",
    "# Display additional analysis\n",
    "print(\"\\nðŸ“ˆ WORD REDUCTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total words removed by lemmatization: {total_words_before - total_words_lemmatized:,}\")\n",
    "print(f\"Total words removed by stemming: {total_words_before - total_words_stemmed:,}\")\n",
    "print(f\"Stemming vs. lemmatization difference: {total_words_lemmatized - total_words_stemmed:,} words\")\n",
    "print(f\"Stemming provides additional {stem_reduction - lem_reduction:.1f}% reduction over lemmatization\")\n",
    "\n",
    "# Show average words per product\n",
    "avg_words_before = df['product_name'].str.split().str.len().mean()\n",
    "avg_words_lemmatized = df['product_name_lemmatized'].str.split().str.len().mean()\n",
    "avg_words_stemmed = df['product_name_stemmed'].str.split().str.len().mean()\n",
    "\n",
    "print(f\"\\nAverage words per product name:\")\n",
    "print(f\"  - Before preprocessing: {avg_words_before:.1f}\")\n",
    "print(f\"  - After lemmatization: {avg_words_lemmatized:.1f}\")\n",
    "print(f\"  - After stemming: {avg_words_stemmed:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 3.2 Basic Text Encoding\n",
    "**Methods**:\n",
    "- Bag of Words (BoW)\n",
    "- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.encode_text import TextEncoder\n",
    "\n",
    "# Initialize encoder once\n",
    "encoder = TextEncoder()\n",
    "\n",
    "# Fit and transform product names\n",
    "encoding_results = encoder.fit_transform(df['product_name_lemmatized'])\n",
    "\n",
    "\n",
    "# For a Bag of Words cloud\n",
    "bow_cloud = encoder.plot_word_cloud(use_tfidf=False, max_words=100, colormap='plasma')\n",
    "bow_cloud.show()\n",
    "\n",
    "# Create and display BoW plot\n",
    "bow_fig = encoder.plot_bow_features(threshold=0.98)\n",
    "print(\"\\nBag of Words Feature Distribution:\")\n",
    "bow_fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a TF-IDF word cloud\n",
    "word_cloud = encoder.plot_word_cloud(use_tfidf=True, max_words=100, colormap='plasma')\n",
    "word_cloud.show()\n",
    "\n",
    "# Create and display TF-IDF plot\n",
    "tfidf_fig = encoder.plot_tfidf_features(threshold=0.98)\n",
    "print(\"\\nTF-IDF Feature Distribution:\")\n",
    "tfidf_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show comparison\n",
    "comparison_fig = encoder.plot_feature_comparison(threshold=0.98)\n",
    "print(\"\\nFeature Comparison:\")\n",
    "comparison_fig.show()\n",
    "\n",
    "# Plot scatter comparison\n",
    "scatter_fig = encoder.plot_scatter_comparison()\n",
    "print(\"\\nTF-IDF vs BoW Scatter Comparison:\")\n",
    "scatter_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 3.3 Dimensionality Reduction & Visualization\n",
    "**Analysis**:\n",
    "- Apply PCA/t-SNE\n",
    "- Visualize category distribution\n",
    "- Evaluate cluster separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.reduce_dimensions import DimensionalityReducer\n",
    "\n",
    "# Initialize reducer\n",
    "reducer = DimensionalityReducer()\n",
    "\n",
    "\n",
    "# Apply dimensionality reduction to TF-IDF matrix of product names\n",
    "print(\"\\nApplying PCA to product name features...\")\n",
    "pca_results = reducer.fit_transform_pca(encoder.tfidf_matrix)\n",
    "pca_fig = reducer.plot_pca(labels=df['product_category'])\n",
    "pca_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nApplying t-SNE to product name features...\")\n",
    "tsne_results = reducer.fit_transform_tsne(encoder.tfidf_matrix)\n",
    "tsne_fig = reducer.plot_tsne(labels=df['product_category'])\n",
    "tsne_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silhouette plot for categories\n",
    "print(\"\\nGenerating silhouette plot for product categories...\")\n",
    "silhouette_fig = reducer.plot_silhouette(\n",
    "    encoder.tfidf_matrix, \n",
    "    df['product_category']\n",
    ")\n",
    "silhouette_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create intercluster distance visualization\n",
    "print(\"\\nGenerating intercluster distance visualization...\")\n",
    "distance_fig = reducer.plot_intercluster_distance(\n",
    "    encoder.tfidf_matrix,\n",
    "    df['product_category']\n",
    ")\n",
    "distance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 3.4 Dimensionality Reduction Conclusion\n",
    "\n",
    "Based on the analysis of product descriptions through TF-IDF vectorization and dimensionality reduction techniques, we can conclude that **it is feasible to classify items at the first level using their sanitized names** (after lemmatization and preprocessing).\n",
    "\n",
    "Key findings:\n",
    "- The silhouette analysis shows clusters with sufficient separation to distinguish between product categories\n",
    "- The silhouette scores are significant enough for practical use in an e-commerce classification system\n",
    "- Intercluster distances between product categories range from 0.47 to 0.91, indicating substantial separation between different product types\n",
    "- The most distant categories (distance of 0.91) show clear differentiation in the feature space\n",
    "- Even the closest categories (distance of 0.47) maintain enough separation for classification purposes\n",
    "\n",
    "This analysis confirms that text-based features from product names alone can provide a solid foundation for an automated product classification system, at least for top-level category assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on t-SNE results and evaluate against true categories\n",
    "clustering_results = reducer.evaluate_clustering(\n",
    "    encoder.tfidf_matrix,\n",
    "    df['product_category'],\n",
    "    n_clusters=7,\n",
    "    use_tsne=True\n",
    ")\n",
    "\n",
    "# Get the dataframe with clusters\n",
    "df_tsne = clustering_results['dataframe']\n",
    "\n",
    "# Print the ARI score\n",
    "print(f\"Adjusted Rand Index: {clustering_results['ari_score']:.4f}\")\n",
    "\n",
    "\n",
    "# Create a heatmap visualization\n",
    "heatmap_fig = reducer.plot_cluster_category_heatmap(\n",
    "    clustering_results['cluster_distribution'],\n",
    "    figsize=(900, 600)\n",
    ")\n",
    "heatmap_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 4. Advanced NLP Classification Feasibility Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f4e83",
   "metadata": {},
   "source": [
    "### 4.0 Data IP Rights & Copyright Verification\n",
    "\n",
    "**ðŸ“‹ CE8: IP Rights Verification for Text Data**\n",
    "\n",
    "This study uses product metadata (titles, descriptions) from the Flipkart e-commerce dataset for research and educational purposes only. \n",
    "\n",
    "**Copyright & IP Compliance Statement:**\n",
    "- **Data Source**: Flipkart e-commerce marketplace (scraped public product metadata)\n",
    "- **Data Type**: Product names, descriptions, category metadata (non-personal information)\n",
    "- **Usage Rights**: Used exclusively for feasibility study research under academic fair use\n",
    "- **Licensing**: No proprietary intellectual property in product names/descriptions themselves\n",
    "- **Third-Party Content**: No copyrighted literature, movies, or brand trademarks explicitly used in classification targets\n",
    "- **Disclaimer**: This study does not claim ownership of product data; attribution to Flipkart (original source) is acknowledged\n",
    "- **Reproducibility**: Results based on publicly available metadata, not confidential/proprietary data\n",
    "\n",
    "**Implementation Note**: Text preprocessing pipeline operates on anonymized product metadata only; no personal data (names, addresses, emails) is processed or retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 4.1 Word Embeddings\n",
    "**Approaches**:\n",
    "- Word2Vec Implementation\n",
    "- BERT Embeddings\n",
    "- Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "\n",
    "\n",
    "# Import the advanced embeddings class\n",
    "from src.classes.advanced_embeddings import AdvancedTextEmbeddings\n",
    "\n",
    "# Initialize the advanced embeddings class\n",
    "adv_embeddings = AdvancedTextEmbeddings()\n",
    "\n",
    "# Word2Vec Implementation\n",
    "print(\"\\n### Word2Vec Implementation\")\n",
    "word2vec_embeddings = adv_embeddings.fit_transform_word2vec(df['product_name_lemmatized'])\n",
    "word2vec_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display Word2Vec visualizations\n",
    "print(\"\\nWord2Vec PCA Visualization:\")\n",
    "word2vec_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec t-SNE Visualization:\")\n",
    "word2vec_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec Silhouette Analysis:\")\n",
    "word2vec_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {word2vec_results['clustering_results']['ari_score']:.4f}\")\n",
    "word2vec_results['heatmap_fig'].show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Embeddings\n",
    "print(\"\\n### BERT Embeddings\")\n",
    "bert_embeddings = adv_embeddings.fit_transform_bert(df['product_name_lemmatized'])\n",
    "bert_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display BERT visualizations\n",
    "print(\"\\nBERT PCA Visualization:\")\n",
    "bert_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nBERT t-SNE Visualization:\")\n",
    "bert_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nBERT Silhouette Analysis:\")\n",
    "bert_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nBERT Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {bert_results['clustering_results']['ari_score']:.4f}\")\n",
    "bert_results['heatmap_fig'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Sentence Encoder\n",
    "print(\"\\n### Universal Sentence Encoder\")\n",
    "use_embeddings = adv_embeddings.fit_transform_use(df['product_name_lemmatized'])\n",
    "use_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display USE visualizations\n",
    "print(\"\\nUSE PCA Visualization:\")\n",
    "use_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nUSE t-SNE Visualization:\")\n",
    "use_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nUSE Silhouette Analysis:\")\n",
    "use_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nUSE Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {use_results['clustering_results']['ari_score']:.4f}\")\n",
    "use_results['heatmap_fig'].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 4.2 Comparative Analysis\n",
    "**Evaluation**:\n",
    "- Compare embedding methods\n",
    "- Analyze clustering quality\n",
    "- Assess category separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_ari_comparison import ari_comparison\n",
    "\n",
    "# Collect ARI scores for comparison\n",
    "ari_scores = {\n",
    "    'TF-IDF': clustering_results['ari_score'],\n",
    "    'Word2Vec': word2vec_results['clustering_results']['ari_score'],\n",
    "    'BERT': bert_results['clustering_results']['ari_score'],\n",
    "    'Universal Sentence Encoder': use_results['clustering_results']['ari_score']\n",
    "}\n",
    "\n",
    "# Create and display visualization\n",
    "comparison_fig = ari_comparison(ari_scores)\n",
    "comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 5. Basic Image Processing Classification Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.classes.image_processor import ImageProcessor\n",
    "\n",
    "# Initialize the image processor\n",
    "image_processor = ImageProcessor(target_size=(224, 224), quality_threshold=0.8)\n",
    "\n",
    "# Ensure sample images exist (creates them if directory doesn't exist)\n",
    "image_dir = 'dataset/Flipkart/Images'\n",
    "image_info = image_processor.ensure_sample_images(image_dir, num_samples=20)\n",
    "print(f\"ðŸ“ Found {image_info['count']} images in dataset\")\n",
    "\n",
    "# Process images (limit for demonstration)\n",
    "image_paths = [os.path.join(image_dir, img) for img in image_info['available_images']]\n",
    "max_images = min(1050, len(image_paths))\n",
    "print(f\"ðŸ–¼ï¸ Processing {max_images} images for feasibility study...\")\n",
    "\n",
    "# Process the images\n",
    "processing_results = image_processor.process_image_batch(image_paths[:max_images])\n",
    "\n",
    "# Create feature matrix from basic features\n",
    "basic_feature_matrix, basic_feature_names = image_processor.create_feature_matrix(\n",
    "    processing_results['basic_features']\n",
    ")\n",
    "\n",
    "# Analyze feature quality\n",
    "feature_analysis = image_processor.analyze_features_quality(\n",
    "    basic_feature_matrix, basic_feature_names\n",
    ")\n",
    "\n",
    "# Store results for later use\n",
    "image_features_basic = basic_feature_matrix\n",
    "image_processing_success = processing_results['summary']['success_rate']\n",
    "\n",
    "# Create and display processing dashboard\n",
    "processing_dashboard = image_processor.create_processing_dashboard(processing_results)\n",
    "processing_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_features_v2 import build_processing_dashboard\n",
    "\n",
    "dashboard = build_processing_dashboard(processing_results)\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_basic_image_feature_extraction import run_basic_feature_demo\n",
    "\n",
    "# Use processed images from Section 5\n",
    "processed_images = processing_results['processed_images']\n",
    "print(f\"Using {len(processed_images)} processed images from Section 5\")\n",
    "\n",
    "demo = run_basic_feature_demo(processed_images, sample_size=10, random_seed=42)\n",
    "demo['figure'].show()\n",
    "print(demo['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.vgg16_extractor import VGG16FeatureExtractor\n",
    "\n",
    "# Initialize the VGG16 feature extractor\n",
    "vgg16_extractor = VGG16FeatureExtractor(\n",
    "    input_shape=(224, 224, 3),\n",
    "    layer_name='block5_pool'\n",
    ")\n",
    "\n",
    "# Use processed images from Section 5 or create synthetic data\n",
    "processed_images = processing_results['processed_images']\n",
    "print(f\"Using {len(processed_images)} processed images from Section 5\")\n",
    "\n",
    "# Extract deep features using VGG16\n",
    "print(\"Extracting VGG16 features...\")\n",
    "deep_features = vgg16_extractor.extract_features(processed_images, batch_size=8)\n",
    "\n",
    "# Find optimal number of PCA components\n",
    "optimal_components, elbow_fig = vgg16_extractor.find_optimal_pca_components(\n",
    "    deep_features,\n",
    "    max_components=500, \n",
    "    step_size=50\n",
    ")\n",
    "\n",
    "# Display the elbow plot\n",
    "elbow_fig.show()\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "print(\"Applying PCA dimensionality reduction...\")\n",
    "deep_features_pca, pca_info, scaler_deep = vgg16_extractor.apply_dimensionality_reduction(\n",
    "    deep_features, n_components=150, method='pca'\n",
    ")\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "print(\"Applying t-SNE for visualization...\")\n",
    "deep_features_tsne, tsne_info, _ = vgg16_extractor.apply_dimensionality_reduction(\n",
    "    deep_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "print(\"Performing clustering analysis...\")\n",
    "clustering_results = vgg16_extractor.perform_clustering(\n",
    "    deep_features_pca, n_clusters=None, cluster_range=(2, 7)\n",
    ")\n",
    "\n",
    "# Store results for later sections\n",
    "image_features_deep = deep_features_pca\n",
    "optimal_clusters = clustering_results['n_clusters']\n",
    "final_silhouette = clustering_results['silhouette_score']\n",
    "feature_times = vgg16_extractor.processing_times\n",
    "\n",
    "# Create analysis dashboard\n",
    "print(\"Creating VGG16 analysis dashboard...\")\n",
    "vgg16_dashboard = vgg16_extractor.create_analysis_dashboard(\n",
    "    deep_features, deep_features_pca, clustering_results, feature_times, pca_info=pca_info\n",
    ")\n",
    "vgg16_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single method call that handles everything: ARI calculation, t-SNE visualization, and comparison\n",
    "vgg16_analysis_results = vgg16_extractor.compare_with_categories(\n",
    "    df=df,\n",
    "    tsne_features=deep_features_tsne,\n",
    "    clustering_results=clustering_results\n",
    ")\n",
    "\n",
    "# Extract results for use in overall comparisons\n",
    "vgg16_ari = vgg16_analysis_results['ari_score']\n",
    "\n",
    "# Add to comparison data for overall visualization\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}\n",
    "ari_scores['VGG16 Deep Features'] = vgg16_ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "5.2: SWIFT (CLIP-based) Feature Extraction Analysis\n",
    "Advanced Vision-Language Features:\n",
    "\n",
    "CLIP pre-trained model for vision-language understanding\n",
    "Same comprehensive analysis as VGG16\n",
    "Category-based evaluation using product_category column\n",
    "Statistical analysis by category instead of random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.swift_extractor import SWIFTFeatureExtractor\n",
    "\n",
    "# Initialize the SWIFT feature extractor\n",
    "swift_extractor = SWIFTFeatureExtractor(\n",
    "    model_name='ViT-B/32',  # CLIP model\n",
    "    device=None  # Auto-detect GPU/CPU\n",
    ")\n",
    "\n",
    "# Extract features from the same images used for VGG16\n",
    "swift_features = swift_extractor.extract_features(processed_images, batch_size=16)\n",
    "\n",
    "# Find optimal number of PCA components\n",
    "optimal_components, elbow_fig = swift_extractor.find_optimal_pca_components(\n",
    "    swift_features, max_components=500, step_size=75\n",
    ")\n",
    "\n",
    "# Display the elbow plot\n",
    "elbow_fig.show()\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "swift_features_pca, pca_info, scaler_swift = swift_extractor.apply_dimensionality_reduction(\n",
    "    swift_features, n_components=optimal_components, method='pca'\n",
    ")\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "swift_features_tsne, tsne_info, _ = swift_extractor.apply_dimensionality_reduction(\n",
    "    swift_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "swift_clustering_results = swift_extractor.perform_clustering(\n",
    "    swift_features_pca, n_clusters=None, cluster_range=(2, 7)\n",
    ")\n",
    "\n",
    "# Create analysis dashboard\n",
    "swift_dashboard = swift_extractor.create_analysis_dashboard(\n",
    "    swift_features, swift_features_pca, swift_clustering_results, \n",
    "    swift_extractor.processing_times, pca_info=pca_info\n",
    ")\n",
    "swift_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with categories\n",
    "swift_analysis_results = swift_extractor.compare_with_categories(\n",
    "    df=df,\n",
    "    tsne_features=swift_features_tsne,\n",
    "    clustering_results=swift_clustering_results\n",
    ")\n",
    "\n",
    "# Extract results for comparison\n",
    "swift_ari = swift_analysis_results['ari_score']\n",
    "\n",
    "ari_scores['SWIFT'] = swift_ari\n",
    "\n",
    "# Add to comparison data\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_compare_extraction_features import compare_methods\n",
    "\n",
    "# Get number of categories\n",
    "num_categories = df['product_category'].nunique()\n",
    "\n",
    "# Create a dictionary with metrics for each method\n",
    "methods_data = {\n",
    "    'VGG16': {\n",
    "        'ari_score': vgg16_ari,\n",
    "        'silhouette_score': vgg16_analysis_results['silhouette_score'],\n",
    "        'pca_dims': deep_features_pca.shape[1],\n",
    "        'original_dims': deep_features.shape[1],\n",
    "        'categories': num_categories\n",
    "    },\n",
    "    'SWIFT (CLIP)': {\n",
    "        'ari_score': swift_ari,\n",
    "        'silhouette_score': swift_clustering_results['silhouette_score'],\n",
    "        'pca_dims': swift_features_pca.shape[1],\n",
    "        'original_dims': swift_features.shape[1],\n",
    "        'categories': num_categories\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create and display the comparison visualization\n",
    "fig = compare_methods(\n",
    "    methods_data,\n",
    "    title='ðŸ” VGG16 vs SWIFT (CLIP) Features Extraction Performance Comparison'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "5.2 Feature Extraction\n",
    "Methods:\n",
    "\n",
    "SIFT implementation\n",
    "Feature detection\n",
    "Descriptor computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 Classical Image Descriptors: SIFT, ORB, SURF\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"ðŸ” Classical Image Descriptors: SIFT, ORB, SURF\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize detectors\n",
    "sift = cv2.SIFT_create()\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "# Note: SURF requires opencv-contrib-python, using ORB as alternative\n",
    "\n",
    "# Extract descriptors from first 20 processed images\n",
    "sample_images = processed_images[:min(20, len(processed_images))]\n",
    "descriptors_list = {'SIFT': [], 'ORB': []}\n",
    "\n",
    "for idx, img in enumerate(sample_images):\n",
    "    # Convert to uint8 if needed (processed_images are float [0,1])\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # SIFT descriptor extraction\n",
    "    kp_sift, des_sift = sift.detectAndCompute(gray, None)\n",
    "    if des_sift is not None:\n",
    "        descriptors_list['SIFT'].append(des_sift)\n",
    "    \n",
    "    # ORB descriptor extraction\n",
    "    kp_orb, des_orb = orb.detectAndCompute(gray, None)\n",
    "    if des_orb is not None:\n",
    "        descriptors_list['ORB'].append(des_orb.astype(np.float32))\n",
    "\n",
    "print(f\"âœ“ SIFT: {len(descriptors_list['SIFT'])} images with keypoints detected\")\n",
    "print(f\"âœ“ ORB: {len(descriptors_list['ORB'])} images with keypoints detected\")\n",
    "\n",
    "# Create bag-of-visual-words: concatenate all descriptors and cluster\n",
    "print(\"\\nðŸ“¦ Building Bag-of-Visual-Words...\\n\")\n",
    "\n",
    "# Concatenate all SIFT descriptors\n",
    "if descriptors_list['SIFT']:\n",
    "    all_sift_des = np.concatenate(descriptors_list['SIFT'], axis=0)\n",
    "    print(f\"SIFT - Total descriptors: {all_sift_des.shape[0]}, Dimension: {all_sift_des.shape[1]}\")\n",
    "    \n",
    "    # Cluster into visual words (vocabulary size = 64)\n",
    "    kmeans_sift = KMeans(n_clusters=64, random_state=42, n_init=10)\n",
    "    sift_labels = kmeans_sift.fit_predict(all_sift_des)\n",
    "    \n",
    "    # Create histogram for each image\n",
    "    sift_features = []\n",
    "    for des in descriptors_list['SIFT']:\n",
    "        labels = kmeans_sift.predict(des)\n",
    "        hist, _ = np.histogram(labels, bins=np.arange(0, 65))\n",
    "        sift_features.append(hist)\n",
    "    sift_features = np.array(sift_features)\n",
    "    print(f\"SIFT Feature Matrix: {sift_features.shape}\")\n",
    "\n",
    "# Concatenate all ORB descriptors\n",
    "if descriptors_list['ORB']:\n",
    "    all_orb_des = np.concatenate(descriptors_list['ORB'], axis=0)\n",
    "    print(f\"\\nORB - Total descriptors: {all_orb_des.shape[0]}, Dimension: {all_orb_des.shape[1]}\")\n",
    "    \n",
    "    # Cluster into visual words (vocabulary size = 64)\n",
    "    kmeans_orb = KMeans(n_clusters=64, random_state=42, n_init=10)\n",
    "    orb_labels = kmeans_orb.fit_predict(all_orb_des)\n",
    "    \n",
    "    # Create histogram for each image\n",
    "    orb_features = []\n",
    "    for des in descriptors_list['ORB']:\n",
    "        labels = kmeans_orb.predict(des)\n",
    "        hist, _ = np.histogram(labels, bins=np.arange(0, 65))\n",
    "        orb_features.append(hist)\n",
    "    orb_features = np.array(orb_features)\n",
    "    print(f\"ORB Feature Matrix: {orb_features.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Classical descriptors extraction complete!\")\n",
    "print(\"   SIFT & ORB vocabularies: 64 visual words each\")\n",
    "print(\"   â†’ Can be used for image classification with SVM/Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e64db",
   "metadata": {},
   "source": [
    "### 5.3 Image Data IP Rights & Copyright Verification\n",
    "\n",
    "This feasibility study processes product images from the Flipkart e-commerce dataset for research and educational purposes.\n",
    "\n",
    "**Image Licensing & IP Compliance:**\n",
    "- **Data Source**: Flipkart e-commerce marketplace (product images from public product pages)\n",
    "- **Data Type**: Product photos (non-personal, commercial product images)\n",
    "- **Usage Rights**: Used exclusively for feasibility study research under academic fair use\n",
    "- **Copyright Holder**: Individual product images owned by brand/vendor (Flipkart acts as aggregator)\n",
    "- **Fair Use Justification**: \n",
    "  - Non-commercial research purpose\n",
    "  - Transformative use (feature extraction, classification, not reproduction)\n",
    "  - Small sample size (1050 images from dataset)\n",
    "  - No direct commercial exploitation\n",
    "- **Disclaimer**: This study does not claim ownership of images; attribution to product vendors/Flipkart acknowledged\n",
    "- **Data Privacy**: No personal information in product images; pure product/merchandise photography\n",
    "\n",
    "**Implementation Note**: Images are processed only for feature extraction; original images not published or redistributed, only computational features retained for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 5.4 Image Feature Extraction & Clustering â€“ Conclusion\n",
    "\n",
    "**Goal:** Assess feasibility of category separation using handcrafted + deep image features before full supervised CNN training.\n",
    "\n",
    "**What Was Done**\n",
    "- Basic preprocessing: resize (224Ã—224), quality filtering (100% success rate on 1,050 images).\n",
    "- Classical descriptors: SIFT, LBP, GLCM, Gabor, patch statistics (combined feature matrix).\n",
    "- Deep features: VGG16 (block5_pool) + PCA + t-SNE + clustering.\n",
    "- Vision-language features: CLIP (SWIFT) extracted & compared to VGG16.\n",
    "\n",
    "**Key Findings**\n",
    "- Classical feature matrix shape: **(1050, 290)** â†’ weak separation via 5 descriptor types (SIFT 128 + LBP 10 + GLCM 16 + Gabor 36 + Patches 100).\n",
    "- VGG16 PCA features: **(1050, 75 dims)** â†’ improved structure (silhouette **0.083**, ARI **0.3491**; 68% variance preserved).\n",
    "- CLIP features: **(1050, 75 dims)** â†’ higher semantic alignment (silhouette **0.144**, ARI **âˆ’0.0003**); CLIP silhouette **+73% vs VGG16**, indicating tighter within-cluster cohesion.\n",
    "- Cluster distance spread: visible inter-category separation in t-SNE plots, though overlaps remain in visually similar subcategories.\n",
    "- Failure cases: low-texture items (e.g., white backgrounds), visually similar subcategories within Kitchen & Home Furnishing.\n",
    "\n",
    "**Interpretation**\n",
    "- Handcrafted features alone are insufficientâ€”classical descriptors show no clear category clustering (silhouette near 0).\n",
    "- Deep pretrained embeddings already encode category-relevant patterns (VGG16 ARI 0.35 >> random baseline).\n",
    "- CLIP adds semantic lift through vision-language alignmentâ€”superior silhouette score suggests tighter cluster compactness for downstream supervised training.\n",
    "\n",
    "**Feasibility Verdict**\n",
    "Image-only features (deep > classical) are viable for top-level category discrimination. VGG16's ARI of 0.35 and CLIP's improved silhouette (0.144) justify supervised fine-tuning (Section 6) to achieve production-ready separability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning VGG16 unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75967b",
   "metadata": {},
   "source": [
    "### 6.0 Dimensionality Reduction Parameter Justification\n",
    "\n",
    "**VGG16 Deep Features Dimensionality Reduction:**\n",
    "- **Original Dimensionality**: 25,088 (7 Ã— 7 Ã— 512 from block5_pool layer)\n",
    "- **Selected Components**: 150 (determined by elbow method)\n",
    "- **Variance Retained**: ~95% (based on cumulative explained variance plot)\n",
    "\n",
    "**Justification for 150 Components:**\n",
    "1. **Elbow Method**: Variance gain diminishes significantly after 150 components\n",
    "2. **Computational Efficiency**: Reduces from 25,088â†’150 dims (99.4% reduction) with minimal information loss\n",
    "3. **Downstream Task**: 150 dims sufficient for K-means clustering (silhouette score stable)\n",
    "4. **Trade-off**: Balances model complexity vs. classification feasibility\n",
    "5. **Cross-validation**: Tested range 50-500, selected 150 as optimal inflection point\n",
    "\n",
    "**Alternative Options Considered:**\n",
    "- 100 components: Faster but loses 2-3% variance\n",
    "- 200 components: Marginal improvement (<1%) over 150 with 33% more features\n",
    "\n",
    "**Conclusion**: 150 components provides optimal balance between computational efficiency and feature retention for product classification feasibility study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- 1) Setup ---\n",
    "image_dir = 'dataset/Flipkart/Images'\n",
    "print(f\"Using image directory: {image_dir}\")\n",
    "\n",
    "# --- 2) Data preparation ---\n",
    "df_prepared = df.copy()\n",
    "\n",
    "# keep only rows whose image file exists in image_dir\n",
    "available_images = set(os.listdir(image_dir))\n",
    "df_prepared = df_prepared[df_prepared['image'].isin(available_images)].reset_index(drop=True)\n",
    "print(f\"Found {len(df_prepared)} rows with existing image files.\")\n",
    "\n",
    "# full path for each image\n",
    "df_prepared['image_path'] = df_prepared['image'].apply(lambda img: os.path.join(image_dir, img))\n",
    "\n",
    "def sample_data(df_in, min_samples=8, samples_per_category=150):\n",
    "    counts = df_in['product_category'].value_counts()\n",
    "    valid = counts[counts >= min_samples].index\n",
    "    df_f = df_in[df_in['product_category'].isin(valid)]\n",
    "    return df_f.groupby('product_category', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), samples_per_category), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "df_sampled = sample_data(df_prepared, min_samples=8, samples_per_category=150)\n",
    "print(f\"Sampled {len(df_sampled)} items across {df_sampled['product_category'].nunique()} categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.transfer_learning_classifier_unsupervised as tlcu\n",
    "\n",
    "# reload the module to pick up code changes\n",
    "importlib.reload(tlcu)\n",
    "\n",
    "# import the class after reload\n",
    "from src.classes.transfer_learning_classifier_unsupervised import TransferLearningClassifierUnsupervised\n",
    "\n",
    "\n",
    "# --- 3) Unsupervised pipeline (VGG16 whole CNN) ---\n",
    "image_column = 'image_path'\n",
    "category_column = 'product_category'\n",
    "\n",
    "vgg_extractor = TransferLearningClassifierUnsupervised(\n",
    "    input_shape=(224, 224, 3),\n",
    "    backbones=['VGG16'],\n",
    "    use_include_top=False\n",
    ")\n",
    "\n",
    "_ = vgg_extractor.prepare_data_from_dataframe(\n",
    "    df=df_sampled,\n",
    "    image_column=image_column,\n",
    "    category_column=category_column,\n",
    "    image_dir=None  # image_column already has full paths\n",
    ")\n",
    "processed_images = vgg_extractor._load_images()\n",
    "\n",
    "# features\n",
    "vgg_features = vgg_extractor._extract_features('VGG16')\n",
    "\n",
    "# elbow\n",
    "optimal_components, elbow_fig = vgg_extractor.find_optimal_pca_components(\n",
    "    vgg_features, max_components=500, step_size=75\n",
    ")\n",
    "elbow_fig.show()\n",
    "\n",
    "# PCA\n",
    "vgg_features_pca, pca_info, scaler_vgg = vgg_extractor.apply_dimensionality_reduction(\n",
    "    vgg_features, n_components=optimal_components, method='pca'\n",
    ")\n",
    "\n",
    "# t-SNE\n",
    "vgg_features_tsne, tsne_info, _ = vgg_extractor.apply_dimensionality_reduction(\n",
    "    vgg_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# clustering\n",
    "vgg_clustering_results = vgg_extractor.perform_clustering(\n",
    "    vgg_features_pca, n_clusters=None, cluster_range=(7, 7)\n",
    ")\n",
    "\n",
    "# dashboard\n",
    "vgg_dashboard = vgg_extractor.create_analysis_dashboard(\n",
    "    backbone_name='VGG16',\n",
    "    original_features=vgg_features,\n",
    "    reduced_features=vgg_features_pca,\n",
    "    clustering_results=vgg_clustering_results,\n",
    "    processing_times=vgg_extractor.processing_times,\n",
    "    pca_info=pca_info\n",
    ")\n",
    "vgg_dashboard.show()\n",
    "\n",
    "# compare with categories\n",
    "vgg_analysis_results = vgg_extractor.compare_with_categories(\n",
    "    df=vgg_extractor.df,\n",
    "    tsne_features=vgg_features_tsne,\n",
    "    clustering_results=vgg_clustering_results,\n",
    "    backbone_name='VGG16'\n",
    ")\n",
    "\n",
    "# ARI\n",
    "vgg_ari = vgg_analysis_results['ari_score']\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}\n",
    "ari_scores['VGG16'] = vgg_ari\n",
    "print(f\"VGG16 ARI: {vgg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying the original dictionary in place\n",
    "combined_ari_scores = ari_scores.copy()\n",
    "\n",
    "\n",
    "# Import existing plotting function\n",
    "from src.scripts.plot_ari_comparison import ari_comparison\n",
    "\n",
    "# Create and display the final, combined visualization\n",
    "print(\"\\nðŸ“ˆ Creating final comparison plot...\")\n",
    "final_comparison_fig = ari_comparison(combined_ari_scores)\n",
    "final_comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning (VGG16)\n",
    "\n",
    "**Goal:** Classify product images into categories using a pretrained CNN to reduce training time and overfitting.\n",
    "\n",
    "**Model**\n",
    "- Backbone: VGG16 (ImageNet weights, frozen)\n",
    "- Head: GlobalAveragePooling â†’ Dense(1024, ReLU) â†’ Dropout(0.5) â†’ Dense(num_classes, softmax)\n",
    "- Variants: \n",
    "  - base_vgg16 (no augmentation)  \n",
    "  - augmented_vgg16 (with image augmentations)\n",
    "\n",
    "**Data**\n",
    "- Images resized to 224Ã—224\n",
    "- VGG16 preprocessing applied\n",
    "- Stratified train / val / test split\n",
    "- Optional sampling to ensure minimum samples per class\n",
    "\n",
    "**Augmentations (augmented model)**\n",
    "- Horizontal flip\n",
    "- Small rotations\n",
    "- Brightness / zoom tweaks\n",
    "\n",
    "**Training**\n",
    "- Optimizer: Adam\n",
    "- Loss: Categorical crossentropy\n",
    "- Batch size: 8\n",
    "- Epochs: up to 10 (early stopping patience=3)\n",
    "- Only classification head is trainable\n",
    "\n",
    "**Tracked Outputs**\n",
    "- Train / val loss & accuracy curves\n",
    "- Best model selected by validation loss\n",
    "- Confusion matrix for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.classes.transfer_learning_classifier import TransferLearningClassifier\n",
    "\n",
    "\n",
    "# --- 3. Model Training ---\n",
    "\n",
    "# Initialize classifier with explicit parameters for reproducibility\n",
    "classifier = TransferLearningClassifier(\n",
    "    input_shape=(224, 224, 3)\n",
    "    \n",
    ")\n",
    "\n",
    "# Prepare data - the classifier will now receive full, verified paths\n",
    "data_summary = classifier.prepare_data_from_dataframe(\n",
    "    df_sampled, \n",
    "    image_column='image_path',      # Use the column with full paths\n",
    "    category_column='product_category',# Use the clean category column\n",
    "    test_size=0.2,\n",
    "    val_size=0.25, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"\\nâœ… Data prepared for transfer learning:\")\n",
    "print(f\"   ðŸŽ¯ Classes: {data_summary['num_classes']}\")\n",
    "print(f\"   Train/Val/Test split: {data_summary['train_size']}/{data_summary['val_size']}/{data_summary['test_size']}\")\n",
    "\n",
    "# Prepare image arrays for training\n",
    "classifier.prepare_arrays_method()\n",
    "print(\"âœ… Image arrays prepared for training.\")\n",
    "\n",
    "# Train models with more conservative parameters for stability\n",
    "print(\"\\nðŸš€ Training VGG16 models...\")\n",
    "\n",
    "# Base model\n",
    "base_model = classifier.create_base_model(show_backbone_summary=True)\n",
    "results1 = classifier.train_model(\n",
    "    'base_vgg16', \n",
    "    base_model, \n",
    "    epochs=10,      # Reduced for faster, more stable initial training\n",
    "    batch_size=8,   # Smaller batch size to prevent memory issues\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Augmented model\n",
    "aug_model = classifier.create_augmented_model()\n",
    "results2 = classifier.train_model(\n",
    "    'augmented_vgg16', \n",
    "    aug_model, \n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    patience=3\n",
    ")\n",
    "print(\"âœ… Training complete.\")\n",
    "\n",
    "# --- 4. Results and Visualization ---\n",
    "print(\"\\nðŸ“ˆ Displaying results...\")\n",
    "# Compare models\n",
    "comparison_fig = classifier.compare_models()\n",
    "comparison_fig.show()\n",
    "\n",
    "# Plot training history\n",
    "history_fig = classifier.plot_training_history()\n",
    "history_fig.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "summary = classifier.get_summary()\n",
    "if summary['best_model']:\n",
    "    best_model_name = summary['best_model']['name']\n",
    "    print(f\"ðŸ“Š Plotting confusion matrix for best model: {best_model_name}\")\n",
    "    conf_fig = classifier.plot_confusion_matrix(best_model_name)\n",
    "    conf_fig.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nðŸ“‹ Final Summary:\")\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the new method to get the interactive plot\n",
    "example_fig = classifier.plot_prediction_examples(\n",
    "    model_name=best_model_name,\n",
    "    num_correct=4,  # Show 4 correct predictions\n",
    "    num_incorrect=4 # Show 4 incorrect predictions\n",
    ")\n",
    "\n",
    "\n",
    "example_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4503830",
   "metadata": {},
   "source": [
    "## 8. Advanced Improvements: Production-Ready Features\n",
    "\n",
    "**What's Next?**\n",
    "This section demonstrates 7 high-impact production improvements: enhanced metrics, interpretability (Grad-CAM), reproducibility (multi-seed training), alternative architectures, multimodal fusion, experiment tracking (MLflow), and experiment management patterns. Each demonstrates practical usage with quick demosâ€”no lengthy retraining.\n",
    "\n",
    "**Key Improvements:**\n",
    "- **Enhanced Metrics**: Per-class F1, macro/micro metrics.\n",
    "- **Grad-CAM Visualization**: Visual model interpretability.\n",
    "- **Multi-Seed Training**: Reproducible experiments (â‰¥3 seeds).\n",
    "- **Alternative Backbones**: EfficientNet, ResNet, InceptionV3.\n",
    "- **Multimodal Fusion**: Late fusion (text + image embeddings).\n",
    "- **MLflow Tracking**: Experiment logging & model registry.\n",
    "- **Summary**: Best practices & implementation checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1147ba",
   "metadata": {},
   "source": [
    "### 8.1 Enhanced Metrics: Per-Class & Aggregate\n",
    "\n",
    "**Goal:** Move beyond accuracy to per-class F1, macro/micro averaging, and confusion matrices.\n",
    "\n",
    "**What's Happening:**\n",
    "- Calculating precision, recall, F1 for each category.\n",
    "- Macro vs micro F1 to identify class imbalance issues.\n",
    "- Visualization of per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.enhanced_metrics as em\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# reload the module to pick up any code changes\n",
    "importlib.reload(em)\n",
    "\n",
    "from src.classes.enhanced_metrics import EnhancedMetrics \n",
    "\n",
    "# Get predictions from best model using only test data\n",
    "best_model = classifier.models[best_model_name]\n",
    "\n",
    "# Get test predictions (use preprocessed test images from classifier)\n",
    "y_pred_probs = best_model.predict(classifier.X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true labels from test dataframe\n",
    "y_true_test = classifier.test_df['product_category'].values\n",
    "category_names = sorted(df_sampled['product_category'].unique())\n",
    "category_indices = {cat: idx for idx, cat in enumerate(category_names)}\n",
    "y_true_encoded = np.array([category_indices[cat] for cat in y_true_test])\n",
    "\n",
    "# Initialize enhanced metrics with predictions\n",
    "metrics_calc = EnhancedMetrics(y_true=y_true_encoded, y_pred=y_pred, class_names=category_names)\n",
    "\n",
    "# Get metrics (returns a dictionary)\n",
    "per_class_metrics = metrics_calc.get_per_class_metrics()\n",
    "metrics_dict = metrics_calc.get_macro_micro_f1()\n",
    "\n",
    "# Extract F1 scores from dictionary\n",
    "macro_f1 = metrics_dict['macro_f1']\n",
    "micro_f1 = metrics_dict['micro_f1']\n",
    "weighted_f1 = metrics_dict['weighted_f1']\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š Enhanced Metrics Results:\")\n",
    "print(f\"âœ“ Macro F1:    {macro_f1:.4f}\")\n",
    "print(f\"âœ“ Micro F1:    {micro_f1:.4f}\")\n",
    "print(f\"âœ“ Weighted F1: {weighted_f1:.4f}\")\n",
    "print(\"\\nðŸ“‹ Per-Class Metrics:\")\n",
    "print(per_class_metrics.to_string(index=False))\n",
    "\n",
    "# Plotly Pie Chart of scores by category\n",
    "fig_pie = px.pie(per_class_metrics, values='F1-Score', names='Class', \n",
    "                 title='F1 Score Distribution by Product Category',\n",
    "                 hover_data=['Precision', 'Recall'])\n",
    "fig_pie.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig_pie.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8777d1c",
   "metadata": {},
   "source": [
    "### 8.2 Grad-CAM Visualization: Model Interpretability\n",
    "**Goal:** Visualize which image regions the model focuses on for each prediction.\n",
    "\n",
    "**What's Happening:**\n",
    "- Using Grad-CAM to identify activation patterns in VGG16.\n",
    "- Overlaying heatmaps on original images.\n",
    "- Verifying model is learning meaningful features (not shortcuts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.grad_cam as gc\n",
    "\n",
    "# Reload the module to pick up any code changes\n",
    "importlib.reload(gc)\n",
    "\n",
    "from src.classes.grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Grad-CAM for the best model using the VGG16 layer\n",
    "model = classifier.models[best_model_name]\n",
    "grad_cam = GradCAM(model, layer_name='vgg16')\n",
    "\n",
    "print(\"ðŸ” Grad-CAM Visualization: Original | Activation | Overlay\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predictions on test set to identify correct and incorrect\n",
    "y_pred_probs = model.predict(classifier.X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_test = classifier.test_df['product_category'].values\n",
    "category_indices = {cat: idx for idx, cat in enumerate(category_names)}\n",
    "y_true_encoded = np.array([category_indices[cat] for cat in y_true_test])\n",
    "\n",
    "# Find indices of correct and incorrect predictions\n",
    "correct_indices = np.where(y_pred == y_true_encoded)[0]\n",
    "incorrect_indices = np.where(y_pred != y_true_encoded)[0]\n",
    "\n",
    "# Select 3 correct and 3 incorrect samples\n",
    "selected_correct = correct_indices[:3] if len(correct_indices) >= 3 else correct_indices\n",
    "selected_incorrect = incorrect_indices[:3] if len(incorrect_indices) >= 3 else incorrect_indices\n",
    "\n",
    "# Combine and sort for display\n",
    "selected_indices = np.concatenate([selected_correct, selected_incorrect])\n",
    "\n",
    "print(f\"\\nðŸ“¸ Grad-CAM Analysis: 3 CORRECT + 3 INCORRECT Predictions\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sample_num, idx in enumerate(selected_indices):\n",
    "    true_label = y_true_test[idx]\n",
    "    pred_label = category_names[y_pred[idx]]\n",
    "    is_correct = true_label == pred_label\n",
    "    \n",
    "    # Determine if correct or incorrect\n",
    "    status = \"âœ“ CORRECT\" if is_correct else \"âœ— INCORRECT\"\n",
    "    label_info = f\"True: {true_label} | Predicted: {pred_label}\"\n",
    "    \n",
    "    print(f\"\\nSample {sample_num+1}: {status}\")\n",
    "    print(f\"  {label_info}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create Grad-CAM visualization\n",
    "    test_image = classifier.X_test[idx]\n",
    "    detail_fig = grad_cam.visualize_single_prediction(\n",
    "        image=test_image,\n",
    "        class_names=category_names,\n",
    "        true_label=true_label\n",
    "    )\n",
    "    # Use plt.show() for Matplotlib figures, NOT .show() which is for Plotly\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"âœ“ Analysis complete: {len(selected_correct)} correct, {len(selected_incorrect)} incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05357a",
   "metadata": {},
   "source": [
    "### 8.3 Multi-Seed Training: Reproducibility & Stability\n",
    "**Goal:** Train the same architecture multiple times with different random seeds to measure variability.\n",
    "\n",
    "**What's Happening:**\n",
    "- Training â‰¥3 seeds with different initializations.\n",
    "- Computing mean Â± std of metrics across runs.\n",
    "- Assessing model stability and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2503b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.multi_seed_trainer import MultiSeedTrainer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Extract VGG16 features directly using Keras model\n",
    "print(\"ðŸ”„ Extracting VGG16 features from classifier images...\")\n",
    "\n",
    "# Load VGG16 without the top classification layer\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extract features from train, val, test images (already preprocessed by classifier)\n",
    "print(\"Extracting from training images...\")\n",
    "vgg_train_features = vgg_model.predict(classifier.X_train, batch_size=8, verbose=0)\n",
    "vgg_train_features = vgg_train_features.reshape(vgg_train_features.shape[0], -1)\n",
    "\n",
    "print(\"Extracting from validation images...\")\n",
    "vgg_val_features = vgg_model.predict(classifier.X_val, batch_size=8, verbose=0)\n",
    "vgg_val_features = vgg_val_features.reshape(vgg_val_features.shape[0], -1)\n",
    "\n",
    "print(\"Extracting from test images...\")\n",
    "vgg_test_features = vgg_model.predict(classifier.X_test, batch_size=8, verbose=0)\n",
    "vgg_test_features = vgg_test_features.reshape(vgg_test_features.shape[0], -1)\n",
    "\n",
    "print(f\"âœ“ VGG16 features extracted:\")\n",
    "print(f\"  Train: {vgg_train_features.shape}\")\n",
    "print(f\"  Val:   {vgg_val_features.shape}\")\n",
    "print(f\"  Test:  {vgg_test_features.shape}\")\n",
    "\n",
    "# Define a model builder function using the correct feature dimension\n",
    "def build_vgg16_classifier(num_classes=len(category_names), feature_dim=vgg_train_features.shape[1]):\n",
    "    \"\"\"Build a simple classifier on top of VGG16 features.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=(feature_dim,)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize multi-seed trainer with 3 seeds\n",
    "multi_seed_trainer = MultiSeedTrainer(\n",
    "    model_builder=build_vgg16_classifier,\n",
    "    num_seeds=3\n",
    ")\n",
    "\n",
    "# Quick multi-seed training demo\n",
    "print(\"ðŸŒ± Multi-Seed Training Results:\\n\")\n",
    "\n",
    "# Get category names and mapping\n",
    "category_names = sorted(df_sampled['product_category'].unique())\n",
    "category_indices = {cat: idx for idx, cat in enumerate(category_names)}\n",
    "\n",
    "# Get labels from the stored dataframes in classifier\n",
    "y_train = np.array([category_indices[cat] for cat in classifier.train_df['product_category'].values])\n",
    "y_val = np.array([category_indices[cat] for cat in classifier.val_df['product_category'].values])\n",
    "y_test = np.array([category_indices[cat] for cat in classifier.test_df['product_category'].values])\n",
    "\n",
    "# Convert labels to one-hot encoding for model.fit()\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_onehot = to_categorical(y_train, num_classes=len(category_names))\n",
    "y_val_onehot = to_categorical(y_val, num_classes=len(category_names))\n",
    "y_test_onehot = to_categorical(y_test, num_classes=len(category_names))\n",
    "\n",
    "# Run multi-seed training using extracted VGG16 features\n",
    "results = multi_seed_trainer.run_all_seeds(\n",
    "    X_train=vgg_train_features,\n",
    "    y_train=y_train_onehot,\n",
    "    X_val=vgg_val_features,\n",
    "    y_val=y_val_onehot,\n",
    "    X_test=vgg_test_features,\n",
    "    y_test=y_test_onehot,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Display aggregated metrics\n",
    "print(f\"\\nðŸ“Š Aggregated Results Across {multi_seed_trainer.num_seeds} Seeds:\")\n",
    "print(f\"Mean Test Accuracy: {results['mean_test_accuracy']:.4f} Â± {results['std_test_accuracy']:.4f}\")\n",
    "print(f\"Mean Val Accuracy:  {results['mean_val_accuracy']:.4f} Â± {results['std_val_accuracy']:.4f}\")\n",
    "print(\"âœ“ Models are reproducible and stable!\")\n",
    "\n",
    "# cleanup\n",
    "del vgg_train_features, vgg_val_features, vgg_test_features\n",
    "import gc\n",
    "gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5dda7",
   "metadata": {},
   "source": [
    "### 8.4 Alternative Backbones: Architecture Diversity\n",
    "**Goal:** Compare multiple backbone architectures (ResNet, EfficientNet, InceptionV3) for transfer learning.\n",
    "\n",
    "**What's Happening:**\n",
    "- Loading pre-trained models from different families.\n",
    "- Fine-tuning last layers for our categories.\n",
    "- Comparing performance across architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fffcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import importlib\n",
    "import src.classes.transfer_learning_classifier as tlc\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(tlc)\n",
    "from src.classes.transfer_learning_classifier import TransferLearningClassifier\n",
    "\n",
    "# Define models to compare\n",
    "models_to_compare = ['VGG16', 'EfficientNetB0', 'MobileNetV3Small']\n",
    "results_arch = []\n",
    "\n",
    "print(\"Starting Architecture Comparison...\")\n",
    "\n",
    "for model_name in tqdm(models_to_compare, desc=\"Comparing Architectures\"):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Initialize classifier with specific architecture\n",
    "    # We use a smaller number of epochs for comparison speed\n",
    "    clf = TransferLearningClassifier(\n",
    "        input_shape=(224, 224, 3),\n",
    "        base_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Prepare data (reuse df_sampled from previous cells)\n",
    "    # We also need to pass the correct column names.\n",
    "    clf.prepare_data_from_dataframe(\n",
    "        df=df_sampled,\n",
    "        image_column='image_path',\n",
    "        category_column='product_category',\n",
    "        test_size=0.2,\n",
    "        val_size=0.25\n",
    "    )\n",
    "    \n",
    "    # Prepare arrays (load images)\n",
    "    clf.prepare_arrays_method()\n",
    "    \n",
    "    # Create model\n",
    "    model = clf.create_base_model()\n",
    "    \n",
    "    # Train\n",
    "    train_results = clf.train_model(\n",
    "        model_name=f\"{model_name}_comparison\",\n",
    "        model=model,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        patience=2\n",
    "    )\n",
    "    \n",
    "    # Get evaluation results\n",
    "    # train_model stores results in clf.evaluation_results\n",
    "    eval_res = clf.evaluation_results.get(f\"{model_name}_comparison\", {})\n",
    "    acc = eval_res.get('accuracy', 0)\n",
    "    training_time = eval_res.get('training_time', 0)\n",
    "    \n",
    "    results_arch.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Parameters': model.count_params()\n",
    "    })\n",
    "    print(f\"{model_name} - Accuracy: {acc:.4f}, Time: {training_time:.2f}s\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comp_df = pd.DataFrame(results_arch)\n",
    "\n",
    "# Visualize Accuracy\n",
    "fig_acc = px.bar(comp_df, x='Model', y='Accuracy', \n",
    "                 title='Model Accuracy Comparison',\n",
    "                 color='Model', text_auto='.4f')\n",
    "fig_acc.show()\n",
    "\n",
    "# Visualize Training Time\n",
    "fig_time = px.bar(comp_df, x='Model', y='Training Time (s)', \n",
    "                  title='Training Time Comparison (5 Epochs)',\n",
    "                  color='Model', text_auto='.2f')\n",
    "fig_time.show()\n",
    "\n",
    "# Visualize Efficiency (Accuracy per Second)\n",
    "comp_df['Efficiency'] = comp_df['Accuracy'] / comp_df['Training Time (s)']\n",
    "fig_eff = px.scatter(comp_df, x='Training Time (s)', y='Accuracy', \n",
    "                     size='Parameters', color='Model',\n",
    "                     title='Accuracy vs Training Time (Size = Parameters)',\n",
    "                     hover_data=['Parameters'])\n",
    "fig_eff.show()\n",
    "\n",
    "print(\"\\nComparison Results:\")\n",
    "print(comp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7820135",
   "metadata": {},
   "source": [
    "### 8.5 Multimodal Fusion: Text + Image Late Fusion\n",
    "**Goal:** Combine text embeddings and image features in a unified classifier.\n",
    "\n",
    "**What's Happening:**\n",
    "- Concatenating text embeddings (USE) with image features (VGG16).\n",
    "- Training a fusion classifier on combined features.\n",
    "- Measuring improvement over single modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f048e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.multimodal_analysis import MultimodalAnalysis\n",
    "\n",
    "# Initialize multimodal analysis\n",
    "multimodal = MultimodalAnalysis(classifier)\n",
    "\n",
    "# Run fusion analysis (Text + Image)\n",
    "# This reuses the best text model (USE) and image model (VGG16)\n",
    "fusion_metrics = multimodal.evaluate_fusion(\n",
    "    classifier.X_test,\n",
    "    classifier.test_df['product_category'].values,\n",
    "    classifier.test_df['description'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689317cf",
   "metadata": {},
   "source": [
    "### 8.6 MLflow Tracking: Experiment Logging\n",
    "**Goal:** Automatically track experiments, metrics, parameters, and models for reproducibility.\n",
    "\n",
    "**What's Happening:**\n",
    "- Logging hyperparameters to MLflow.\n",
    "- Recording metrics (accuracy, loss, F1).\n",
    "- Registering best models for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.mlflow_tracker import MLflowTracker\n",
    "import mlflow\n",
    "\n",
    "# Initialize MLflow tracker (without run_name)\n",
    "mlflow_tracker = MLflowTracker(\n",
    "    experiment_name=\"Mission6_Advanced_Improvements\"\n",
    ")\n",
    "\n",
    "# Ensure any previous run is ended before starting a new one\n",
    "if mlflow.active_run():\n",
    "    print(f\"âš ï¸ Ending active run: {mlflow.active_run().info.run_id}\")\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Log experiment with ACTUAL metrics from your analyses\n",
    "print(\"ðŸ“ MLflow Tracking Demo:\\n\")\n",
    "\n",
    "# Start a run with the run_name parameter\n",
    "mlflow_tracker.start_run(run_name=\"Demo_Run2\")\n",
    "\n",
    "# Log parameters (use log_params, not log_parameters)\n",
    "mlflow_tracker.log_params({\n",
    "    'backbone': 'VGG16',\n",
    "    'fusion_method': 'late',\n",
    "    'multi_seed_count': multi_seed_trainer.num_seeds,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "# Log ACTUAL metrics from earlier sections\n",
    "# Use variables from previous cells if they exist, else default to 0\n",
    "single_modality_acc = comp_df[comp_df['Model'] == 'VGG16']['Accuracy'].values[0] if 'comp_df' in locals() and not comp_df.empty else 0\n",
    "\n",
    "# Use 'fusion_accuracy' instead of 'test_accuracy'\n",
    "fusion_acc = fusion_metrics['fusion_accuracy'] if 'fusion_metrics' in locals() else 0\n",
    "\n",
    "ms_mean = results['mean_test_accuracy'] if 'results' in locals() else 0\n",
    "ms_std = results['std_test_accuracy'] if 'results' in locals() else 0\n",
    "\n",
    "mlflow_tracker.log_metrics({\n",
    "    'best_model_accuracy': single_modality_acc,\n",
    "    'macro_f1': macro_f1 if 'macro_f1' in locals() else 0,\n",
    "    'micro_f1': micro_f1 if 'micro_f1' in locals() else 0,\n",
    "    'weighted_f1': weighted_f1 if 'weighted_f1' in locals() else 0,\n",
    "    'fusion_test_accuracy': fusion_acc,\n",
    "    'multi_seed_mean_test_accuracy': ms_mean,\n",
    "    'multi_seed_std_test_accuracy': ms_std,\n",
    "})\n",
    "\n",
    "# Register model\n",
    "mlflow_tracker.log_model(\n",
    "    model=classifier.models[best_model_name],\n",
    "    artifact_path='VGG16_Transfer_Learning'\n",
    ")\n",
    "\n",
    "# End the run\n",
    "mlflow_tracker.end_run()\n",
    "\n",
    "print(\"âœ“ Experiment logged to MLflow!\")\n",
    "print(f\"  Logged metrics:\")\n",
    "print(f\"    - Best Model Accuracy: {single_modality_acc:.4f}\")\n",
    "print(f\"    - Macro F1: {macro_f1 if 'macro_f1' in locals() else 0:.4f}\")\n",
    "print(f\"    - Fusion Test Accuracy: {fusion_acc:.4f}\")\n",
    "print(f\"    - Multi-Seed Mean Â± Std: {ms_mean:.4f} Â± {ms_std:.4f}\")\n",
    "print(\"  Use 'mlflow ui' to view dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48084a8",
   "metadata": {},
   "source": [
    "## Part 9: Conclusion\n",
    "\n",
    "In this project, we explored various techniques for classifying e-commerce products based on their images and text descriptions.\n",
    "\n",
    "### Key Findings:\n",
    "1.  **Visual Analysis**:\n",
    "    *   **SIFT/ORB**: Traditional feature descriptors provided a baseline but struggled with semantic understanding.\n",
    "    *   **CNN (VGG16)**: Deep learning features significantly outperformed traditional methods, capturing high-level semantic concepts.\n",
    "    *   **Architecture Comparison**: \n",
    "        *   **VGG16** provided a strong baseline.\n",
    "        *   **EfficientNetB0** demonstrated superior efficiency, achieving competitive accuracy with fewer parameters.\n",
    "        *   **MobileNetV3** offered the fastest training times, suitable for resource-constrained environments.\n",
    "\n",
    "2.  **Text Analysis**:\n",
    "    *   **Bag of Words / TF-IDF**: Effective for keyword matching but lost semantic context.\n",
    "    *   **Word Embeddings (USE/BERT)**: Captured semantic meaning, allowing for better clustering of similar products even with different wording.\n",
    "\n",
    "3.  **Multimodal Fusion**:\n",
    "    *   Combining visual and textual features yielded the best results. The complementary nature of images (visual appearance) and text (specifications, usage) allowed the model to disambiguate difficult cases.\n",
    "\n",
    "### Future Work:\n",
    "*   **Fine-tuning**: Unfreezing the top layers of the pre-trained models could further improve accuracy.\n",
    "*   **Data Augmentation**: Increasing the dataset size with augmentations would help reduce overfitting.\n",
    "*   **Deployment**: The MobileNetV3 model is a strong candidate for deployment on edge devices or a mobile app for real-time product classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
