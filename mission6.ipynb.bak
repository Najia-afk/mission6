{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mission 6: Feasibility Study of Product Classification Engine\n",
    "\n",
    "## 1. Introduction\n",
    "**Objective**: Evaluate the feasibility of automatic product classification using text descriptions and images for an e-commerce marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "### 2.1 Components\n",
    "| Modality | Description | Source | Notes |\n",
    "|----------|-------------|--------|-------|\n",
    "| Images | Product photos (RGB) | Flipkart dataset | Variable resolutions; resized to 224√ó224 |\n",
    "| Text | Product titles / descriptions (English) | Metadata CSV | Cleaned: lowercased, punctuation stripped, stopwords partially removed |\n",
    "| Labels | Product category identifiers | Metadata CSV | Multi-class (N classes) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Plotly to properly render in HTML exports\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set the renderer for notebook display\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Configure global theme for consistent appearance\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Read all CSV files from dataset/Flipkart directory with glob\n",
    "csv_files = glob.glob('dataset/Flipkart/flipkart*.csv')\n",
    "\n",
    "# Import the CSV files into a dataframe\n",
    "df = pd.read_csv(csv_files[0])\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2.2 Basic Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.analyze_value_specifications import SpecificationsValueAnalyzer\n",
    "\n",
    "analyzer = SpecificationsValueAnalyzer(df)\n",
    "value_analysis = analyzer.get_top_values(top_keys=5, top_values=5)\n",
    "value_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Class Balance (Post-Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radial icicle chart to visualize the top values\n",
    "fig = analyzer.create_radial_icicle_chart(top_keys=10, top_values=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.analyze_category_tree import CategoryTreeAnalyzer\n",
    "\n",
    "# Create analyzer instance with your dataframe\n",
    "category_analyzer = CategoryTreeAnalyzer(df)\n",
    "\n",
    "# Create and display the radial category chart\n",
    "fig = category_analyzer.create_radial_category_chart(max_depth=9)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Basic NLP Classification Feasibility Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 3.1 Text Preprocessing\n",
    "**Steps**:\n",
    "- Clean text data\n",
    "- Remove stopwords\n",
    "- Perform stemming/lemmatization\n",
    "- Handle special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TextPreprocessor class\n",
    "from src.classes.preprocess_text import TextPreprocessor\n",
    "\n",
    "# Create processor instance\n",
    "processor = TextPreprocessor()\n",
    "\n",
    "# 1. Demonstrate functions with a clear example sentence\n",
    "print(\"üîç TEXT PREPROCESSING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_sentence = \"To be or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and, by opposing, end them?\"\n",
    "\n",
    "print(f\"Original: '{test_sentence}'\")\n",
    "print(f\"Tokenized: {processor.tokenize_sentence(test_sentence)}\")\n",
    "print(f\"Stemmed: '{processor.stem_sentence(test_sentence)}'\")\n",
    "print(f\"Lemmatized: '{processor.lemmatize_sentence(test_sentence)}'\")\n",
    "print(f\"Fully preprocessed: '{processor.preprocess(test_sentence)}'\")\n",
    "\n",
    "# 2. Process the DataFrame columns efficiently\n",
    "print(\"\\nüîÑ APPLYING TO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply preprocessing to product names\n",
    "df['product_name_lemmatized'] = df['product_name'].apply(processor.preprocess)\n",
    "df['product_name_stemmed'] = df['product_name'].apply(processor.stem_text)\n",
    "df['product_category'] = df['product_category_tree'].apply(processor.extract_top_category)\n",
    "\n",
    "# 3. Show a few examples of the transformations\n",
    "print(\"\\nüìã TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "comparison_data = []\n",
    "\n",
    "for i in range(min(5, len(df))):\n",
    "    original = df['product_name'].iloc[i]\n",
    "    lemmatized = df['product_name_lemmatized'].iloc[i]\n",
    "    stemmed = df['product_name_stemmed'].iloc[i]\n",
    "    \n",
    "    # Truncate long examples for display\n",
    "    max_len = 50\n",
    "    orig_display = original[:max_len] + ('...' if len(original) > max_len else '')\n",
    "    lem_display = lemmatized[:max_len] + ('...' if len(lemmatized) > max_len else '')\n",
    "    stem_display = stemmed[:max_len] + ('...' if len(stemmed) > max_len else '')\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Original': orig_display,\n",
    "        'Lemmatized': lem_display,\n",
    "        'Stemmed': stem_display\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\nüìä PREPROCESSING STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "total_words_before = df['product_name'].str.split().str.len().sum()\n",
    "total_words_lemmatized = df['product_name_lemmatized'].str.split().str.len().sum()\n",
    "total_words_stemmed = df['product_name_stemmed'].str.split().str.len().sum()\n",
    "\n",
    "lem_reduction = ((total_words_before - total_words_lemmatized) / total_words_before) * 100\n",
    "stem_reduction = ((total_words_before - total_words_stemmed) / total_words_before) * 100\n",
    "\n",
    "print(f\"Total words before processing: {total_words_before:,}\")\n",
    "print(f\"Words after lemmatization: {total_words_lemmatized:,} ({lem_reduction:.1f}% reduction)\")\n",
    "print(f\"Words after stemming: {total_words_stemmed:,} ({stem_reduction:.1f}% reduction)\")\n",
    "print(f\"Unique categories extracted: {df['product_category'].nunique()}\")\n",
    "\n",
    "# Display additional analysis\n",
    "print(\"\\nüìà WORD REDUCTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total words removed by lemmatization: {total_words_before - total_words_lemmatized:,}\")\n",
    "print(f\"Total words removed by stemming: {total_words_before - total_words_stemmed:,}\")\n",
    "print(f\"Stemming vs. lemmatization difference: {total_words_lemmatized - total_words_stemmed:,} words\")\n",
    "print(f\"Stemming provides additional {stem_reduction - lem_reduction:.1f}% reduction over lemmatization\")\n",
    "\n",
    "# Show average words per product\n",
    "avg_words_before = df['product_name'].str.split().str.len().mean()\n",
    "avg_words_lemmatized = df['product_name_lemmatized'].str.split().str.len().mean()\n",
    "avg_words_stemmed = df['product_name_stemmed'].str.split().str.len().mean()\n",
    "\n",
    "print(f\"\\nAverage words per product name:\")\n",
    "print(f\"  - Before preprocessing: {avg_words_before:.1f}\")\n",
    "print(f\"  - After lemmatization: {avg_words_lemmatized:.1f}\")\n",
    "print(f\"  - After stemming: {avg_words_stemmed:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 3.2 Basic Text Encoding\n",
    "**Methods**:\n",
    "- Bag of Words (BoW)\n",
    "- TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.encode_text import TextEncoder\n",
    "\n",
    "# Initialize encoder once\n",
    "encoder = TextEncoder()\n",
    "\n",
    "# Fit and transform product names\n",
    "encoding_results = encoder.fit_transform(df['product_name_lemmatized'])\n",
    "\n",
    "\n",
    "# For a Bag of Words cloud\n",
    "bow_cloud = encoder.plot_word_cloud(use_tfidf=False, max_words=100, colormap='plasma')\n",
    "bow_cloud.show()\n",
    "\n",
    "# Create and display BoW plot\n",
    "bow_fig = encoder.plot_bow_features(threshold=0.98)\n",
    "print(\"\\nBag of Words Feature Distribution:\")\n",
    "bow_fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a TF-IDF word cloud\n",
    "word_cloud = encoder.plot_word_cloud(use_tfidf=True, max_words=100, colormap='plasma')\n",
    "word_cloud.show()\n",
    "\n",
    "# Create and display TF-IDF plot\n",
    "tfidf_fig = encoder.plot_tfidf_features(threshold=0.98)\n",
    "print(\"\\nTF-IDF Feature Distribution:\")\n",
    "tfidf_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show comparison\n",
    "comparison_fig = encoder.plot_feature_comparison(threshold=0.98)\n",
    "print(\"\\nFeature Comparison:\")\n",
    "comparison_fig.show()\n",
    "\n",
    "# Plot scatter comparison\n",
    "scatter_fig = encoder.plot_scatter_comparison()\n",
    "print(\"\\nTF-IDF vs BoW Scatter Comparison:\")\n",
    "scatter_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 3.3 Dimensionality Reduction & Visualization\n",
    "**Analysis**:\n",
    "- Apply PCA/t-SNE\n",
    "- Visualize category distribution\n",
    "- Evaluate cluster separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.reduce_dimensions import DimensionalityReducer\n",
    "\n",
    "# Initialize reducer\n",
    "reducer = DimensionalityReducer()\n",
    "\n",
    "\n",
    "# Apply dimensionality reduction to TF-IDF matrix of product names\n",
    "print(\"\\nApplying PCA to product name features...\")\n",
    "pca_results = reducer.fit_transform_pca(encoder.tfidf_matrix)\n",
    "pca_fig = reducer.plot_pca(labels=df['product_category'])\n",
    "pca_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nApplying t-SNE to product name features...\")\n",
    "tsne_results = reducer.fit_transform_tsne(encoder.tfidf_matrix)\n",
    "tsne_fig = reducer.plot_tsne(labels=df['product_category'])\n",
    "tsne_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silhouette plot for categories\n",
    "print(\"\\nGenerating silhouette plot for product categories...\")\n",
    "silhouette_fig = reducer.plot_silhouette(\n",
    "    encoder.tfidf_matrix, \n",
    "    df['product_category']\n",
    ")\n",
    "silhouette_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create intercluster distance visualization\n",
    "print(\"\\nGenerating intercluster distance visualization...\")\n",
    "distance_fig = reducer.plot_intercluster_distance(\n",
    "    encoder.tfidf_matrix,\n",
    "    df['product_category']\n",
    ")\n",
    "distance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 3.4 Dimensionality Reduction Conclusion\n",
    "\n",
    "Based on the analysis of product descriptions through TF-IDF vectorization and dimensionality reduction techniques, we can conclude that **it is feasible to classify items at the first level using their sanitized names** (after lemmatization and preprocessing).\n",
    "\n",
    "Key findings:\n",
    "- The silhouette analysis shows clusters with sufficient separation to distinguish between product categories\n",
    "- The silhouette scores are significant enough for practical use in an e-commerce classification system\n",
    "- Intercluster distances between product categories range from 0.47 to 0.91, indicating substantial separation between different product types\n",
    "- The most distant categories (distance of 0.91) show clear differentiation in the feature space\n",
    "- Even the closest categories (distance of 0.47) maintain enough separation for classification purposes\n",
    "\n",
    "This analysis confirms that text-based features from product names alone can provide a solid foundation for an automated product classification system, at least for top-level category assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on t-SNE results and evaluate against true categories\n",
    "clustering_results = reducer.evaluate_clustering(\n",
    "    encoder.tfidf_matrix,\n",
    "    df['product_category'],\n",
    "    n_clusters=7,\n",
    "    use_tsne=True\n",
    ")\n",
    "\n",
    "# Get the dataframe with clusters\n",
    "df_tsne = clustering_results['dataframe']\n",
    "\n",
    "# Print the ARI score\n",
    "print(f\"Adjusted Rand Index: {clustering_results['ari_score']:.4f}\")\n",
    "\n",
    "\n",
    "# Create a heatmap visualization\n",
    "heatmap_fig = reducer.plot_cluster_category_heatmap(\n",
    "    clustering_results['cluster_distribution'],\n",
    "    figsize=(900, 600)\n",
    ")\n",
    "heatmap_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 4. Advanced NLP Classification Feasibility Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f4e83",
   "metadata": {},
   "source": [
    "### 4.0 Data IP Rights & Copyright Verification\n",
    "\n",
    "**üìã CE8: IP Rights Verification for Text Data**\n",
    "\n",
    "This study uses product metadata (titles, descriptions) from the Flipkart e-commerce dataset for research and educational purposes only. \n",
    "\n",
    "**Copyright & IP Compliance Statement:**\n",
    "- **Data Source**: Flipkart e-commerce marketplace (scraped public product metadata)\n",
    "- **Data Type**: Product names, descriptions, category metadata (non-personal information)\n",
    "- **Usage Rights**: Used exclusively for feasibility study research under academic fair use\n",
    "- **Licensing**: No proprietary intellectual property in product names/descriptions themselves\n",
    "- **Third-Party Content**: No copyrighted literature, movies, or brand trademarks explicitly used in classification targets\n",
    "- **Disclaimer**: This study does not claim ownership of product data; attribution to Flipkart (original source) is acknowledged\n",
    "- **Reproducibility**: Results based on publicly available metadata, not confidential/proprietary data\n",
    "\n",
    "**Implementation Note**: Text preprocessing pipeline operates on anonymized product metadata only; no personal data (names, addresses, emails) is processed or retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 4.1 Word Embeddings\n",
    "**Approaches**:\n",
    "- Word2Vec Implementation\n",
    "- BERT Embeddings\n",
    "- Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "\n",
    "\n",
    "# Import the advanced embeddings class\n",
    "from src.classes.advanced_embeddings import AdvancedTextEmbeddings\n",
    "\n",
    "# Initialize the advanced embeddings class\n",
    "adv_embeddings = AdvancedTextEmbeddings()\n",
    "\n",
    "# Word2Vec Implementation\n",
    "print(\"\\n### Word2Vec Implementation\")\n",
    "word2vec_embeddings = adv_embeddings.fit_transform_word2vec(df['product_name_lemmatized'])\n",
    "word2vec_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display Word2Vec visualizations\n",
    "print(\"\\nWord2Vec PCA Visualization:\")\n",
    "word2vec_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec t-SNE Visualization:\")\n",
    "word2vec_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec Silhouette Analysis:\")\n",
    "word2vec_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nWord2Vec Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {word2vec_results['clustering_results']['ari_score']:.4f}\")\n",
    "word2vec_results['heatmap_fig'].show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Embeddings\n",
    "print(\"\\n### BERT Embeddings\")\n",
    "bert_embeddings = adv_embeddings.fit_transform_bert(df['product_name_lemmatized'])\n",
    "bert_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display BERT visualizations\n",
    "print(\"\\nBERT PCA Visualization:\")\n",
    "bert_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nBERT t-SNE Visualization:\")\n",
    "bert_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nBERT Silhouette Analysis:\")\n",
    "bert_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nBERT Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {bert_results['clustering_results']['ari_score']:.4f}\")\n",
    "bert_results['heatmap_fig'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Sentence Encoder\n",
    "print(\"\\n### Universal Sentence Encoder\")\n",
    "use_embeddings = adv_embeddings.fit_transform_use(df['product_name_lemmatized'])\n",
    "use_results = adv_embeddings.compare_with_reducer(reducer, df['product_category'])\n",
    "\n",
    "# Display USE visualizations\n",
    "print(\"\\nUSE PCA Visualization:\")\n",
    "use_results['pca_fig'].show()\n",
    "\n",
    "print(\"\\nUSE t-SNE Visualization:\")\n",
    "use_results['tsne_fig'].show()\n",
    "\n",
    "print(\"\\nUSE Silhouette Analysis:\")\n",
    "use_results['silhouette_fig'].show()\n",
    "\n",
    "print(\"\\nUSE Cluster Analysis:\")\n",
    "print(f\"Adjusted Rand Index: {use_results['clustering_results']['ari_score']:.4f}\")\n",
    "use_results['heatmap_fig'].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 4.2 Comparative Analysis\n",
    "**Evaluation**:\n",
    "- Compare embedding methods\n",
    "- Analyze clustering quality\n",
    "- Assess category separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_ari_comparison import ari_comparison\n",
    "\n",
    "# Collect ARI scores for comparison\n",
    "ari_scores = {\n",
    "    'TF-IDF': clustering_results['ari_score'],\n",
    "    'Word2Vec': word2vec_results['clustering_results']['ari_score'],\n",
    "    'BERT': bert_results['clustering_results']['ari_score'],\n",
    "    'Universal Sentence Encoder': use_results['clustering_results']['ari_score']\n",
    "}\n",
    "\n",
    "# Create and display visualization\n",
    "comparison_fig = ari_comparison(ari_scores)\n",
    "comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 5. Basic Image Processing Classification Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.classes.image_processor import ImageProcessor\n",
    "\n",
    "# Initialize the image processor\n",
    "image_processor = ImageProcessor(target_size=(224, 224), quality_threshold=0.8)\n",
    "\n",
    "# Ensure sample images exist (creates them if directory doesn't exist)\n",
    "image_dir = 'dataset/Flipkart/Images'\n",
    "image_info = image_processor.ensure_sample_images(image_dir, num_samples=20)\n",
    "print(f\"üìÅ Found {image_info['count']} images in dataset\")\n",
    "\n",
    "# Process images (limit for demonstration)\n",
    "image_paths = [os.path.join(image_dir, img) for img in image_info['available_images']]\n",
    "max_images = min(1050, len(image_paths))\n",
    "print(f\"üñºÔ∏è Processing {max_images} images for feasibility study...\")\n",
    "\n",
    "# Process the images\n",
    "processing_results = image_processor.process_image_batch(image_paths[:max_images])\n",
    "\n",
    "# Create feature matrix from basic features\n",
    "basic_feature_matrix, basic_feature_names = image_processor.create_feature_matrix(\n",
    "    processing_results['basic_features']\n",
    ")\n",
    "\n",
    "# Analyze feature quality\n",
    "feature_analysis = image_processor.analyze_features_quality(\n",
    "    basic_feature_matrix, basic_feature_names\n",
    ")\n",
    "\n",
    "# Store results for later use\n",
    "image_features_basic = basic_feature_matrix\n",
    "image_processing_success = processing_results['summary']['success_rate']\n",
    "\n",
    "# Create and display processing dashboard\n",
    "processing_dashboard = image_processor.create_processing_dashboard(processing_results)\n",
    "processing_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_features_v2 import build_processing_dashboard\n",
    "\n",
    "dashboard = build_processing_dashboard(processing_results)\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_basic_image_feature_extraction import run_basic_feature_demo\n",
    "\n",
    "# Use processed images from Section 5\n",
    "processed_images = processing_results['processed_images']\n",
    "print(f\"Using {len(processed_images)} processed images from Section 5\")\n",
    "\n",
    "demo = run_basic_feature_demo(processed_images, sample_size=10, random_seed=42)\n",
    "demo['figure'].show()\n",
    "print(demo['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.vgg16_extractor import VGG16FeatureExtractor\n",
    "\n",
    "# Initialize the VGG16 feature extractor\n",
    "vgg16_extractor = VGG16FeatureExtractor(\n",
    "    input_shape=(224, 224, 3),\n",
    "    layer_name='block5_pool'\n",
    ")\n",
    "\n",
    "# Use processed images from Section 5 or create synthetic data\n",
    "processed_images = processing_results['processed_images']\n",
    "print(f\"Using {len(processed_images)} processed images from Section 5\")\n",
    "\n",
    "# Extract deep features using VGG16\n",
    "print(\"Extracting VGG16 features...\")\n",
    "deep_features = vgg16_extractor.extract_features(processed_images, batch_size=8)\n",
    "\n",
    "# Find optimal number of PCA components\n",
    "optimal_components, elbow_fig = vgg16_extractor.find_optimal_pca_components(\n",
    "    deep_features,\n",
    "    max_components=500, \n",
    "    step_size=50\n",
    ")\n",
    "\n",
    "# Display the elbow plot\n",
    "elbow_fig.show()\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "print(\"Applying PCA dimensionality reduction...\")\n",
    "deep_features_pca, pca_info, scaler_deep = vgg16_extractor.apply_dimensionality_reduction(\n",
    "    deep_features, n_components=150, method='pca'\n",
    ")\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "print(\"Applying t-SNE for visualization...\")\n",
    "deep_features_tsne, tsne_info, _ = vgg16_extractor.apply_dimensionality_reduction(\n",
    "    deep_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "print(\"Performing clustering analysis...\")\n",
    "clustering_results = vgg16_extractor.perform_clustering(\n",
    "    deep_features_pca, n_clusters=None, cluster_range=(2, 7)\n",
    ")\n",
    "\n",
    "# Store results for later sections\n",
    "image_features_deep = deep_features_pca\n",
    "optimal_clusters = clustering_results['n_clusters']\n",
    "final_silhouette = clustering_results['silhouette_score']\n",
    "feature_times = vgg16_extractor.processing_times\n",
    "\n",
    "# Create analysis dashboard\n",
    "print(\"Creating VGG16 analysis dashboard...\")\n",
    "vgg16_dashboard = vgg16_extractor.create_analysis_dashboard(\n",
    "    deep_features, deep_features_pca, clustering_results, feature_times, pca_info=pca_info\n",
    ")\n",
    "vgg16_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1219afe",
   "metadata": {},
   "source": [
    "### 6.0 Dimensionality Reduction Parameter Justification\n",
    "\n",
    "**VGG16 Deep Features Dimensionality Reduction:**\n",
    "- **Original Dimensionality**: 25,088 (7 √ó 7 √ó 512 from block5_pool layer)\n",
    "- **Selected Components**: 150 (determined by elbow method)\n",
    "- **Variance Retained**: ~95% (based on cumulative explained variance plot)\n",
    "\n",
    "**Justification for 150 Components:**\n",
    "1. **Elbow Method**: Variance gain diminishes significantly after 150 components\n",
    "2. **Computational Efficiency**: Reduces from 25,088‚Üí150 dims (99.4% reduction) with minimal information loss\n",
    "3. **Downstream Task**: 150 dims sufficient for K-means clustering (silhouette score stable)\n",
    "4. **Trade-off**: Balances model complexity vs. classification feasibility\n",
    "5. **Cross-validation**: Tested range 50-500, selected 150 as optimal inflection point\n",
    "\n",
    "**Alternative Options Considered:**\n",
    "- 100 components: Faster but loses 2-3% variance\n",
    "- 200 components: Marginal improvement (<1%) over 150 with 33% more features\n",
    "\n",
    "**Conclusion**: 150 components provides optimal balance between computational efficiency and feature retention for product classification feasibility study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single method call that handles everything: ARI calculation, t-SNE visualization, and comparison\n",
    "vgg16_analysis_results = vgg16_extractor.compare_with_categories(\n",
    "    df=df,\n",
    "    tsne_features=deep_features_tsne,\n",
    "    clustering_results=clustering_results\n",
    ")\n",
    "\n",
    "# Extract results for use in overall comparisons\n",
    "vgg16_ari = vgg16_analysis_results['ari_score']\n",
    "\n",
    "# Add to comparison data for overall visualization\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}\n",
    "ari_scores['VGG16 Deep Features'] = vgg16_ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "5.2: SWIFT (CLIP-based) Feature Extraction Analysis\n",
    "Advanced Vision-Language Features:\n",
    "\n",
    "CLIP pre-trained model for vision-language understanding\n",
    "Same comprehensive analysis as VGG16\n",
    "Category-based evaluation using product_category column\n",
    "Statistical analysis by category instead of random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.swift_extractor import SWIFTFeatureExtractor\n",
    "\n",
    "# Initialize the SWIFT feature extractor\n",
    "swift_extractor = SWIFTFeatureExtractor(\n",
    "    model_name='ViT-B/32',  # CLIP model\n",
    "    device=None  # Auto-detect GPU/CPU\n",
    ")\n",
    "\n",
    "# Extract features from the same images used for VGG16\n",
    "swift_features = swift_extractor.extract_features(processed_images, batch_size=16)\n",
    "\n",
    "# Find optimal number of PCA components\n",
    "optimal_components, elbow_fig = swift_extractor.find_optimal_pca_components(\n",
    "    swift_features, max_components=500, step_size=75\n",
    ")\n",
    "\n",
    "# Display the elbow plot\n",
    "elbow_fig.show()\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "swift_features_pca, pca_info, scaler_swift = swift_extractor.apply_dimensionality_reduction(\n",
    "    swift_features, n_components=optimal_components, method='pca'\n",
    ")\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "swift_features_tsne, tsne_info, _ = swift_extractor.apply_dimensionality_reduction(\n",
    "    swift_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "swift_clustering_results = swift_extractor.perform_clustering(\n",
    "    swift_features_pca, n_clusters=None, cluster_range=(2, 7)\n",
    ")\n",
    "\n",
    "# Create analysis dashboard\n",
    "swift_dashboard = swift_extractor.create_analysis_dashboard(\n",
    "    swift_features, swift_features_pca, swift_clustering_results, \n",
    "    swift_extractor.processing_times, pca_info=pca_info\n",
    ")\n",
    "swift_dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with categories\n",
    "swift_analysis_results = swift_extractor.compare_with_categories(\n",
    "    df=df,\n",
    "    tsne_features=swift_features_tsne,\n",
    "    clustering_results=swift_clustering_results\n",
    ")\n",
    "\n",
    "# Extract results for comparison\n",
    "swift_ari = swift_analysis_results['ari_score']\n",
    "\n",
    "ari_scores['SWIFT'] = swift_ari\n",
    "\n",
    "# Add to comparison data\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.plot_compare_extraction_features import compare_methods\n",
    "\n",
    "# Get number of categories\n",
    "num_categories = df['product_category'].nunique()\n",
    "\n",
    "# Create a dictionary with metrics for each method\n",
    "methods_data = {\n",
    "    'VGG16': {\n",
    "        'ari_score': vgg16_ari,\n",
    "        'silhouette_score': vgg16_analysis_results['silhouette_score'],\n",
    "        'pca_dims': deep_features_pca.shape[1],\n",
    "        'original_dims': deep_features.shape[1],\n",
    "        'categories': num_categories\n",
    "    },\n",
    "    'SWIFT (CLIP)': {\n",
    "        'ari_score': swift_ari,\n",
    "        'silhouette_score': swift_clustering_results['silhouette_score'],\n",
    "        'pca_dims': swift_features_pca.shape[1],\n",
    "        'original_dims': swift_features.shape[1],\n",
    "        'categories': num_categories\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create and display the comparison visualization\n",
    "fig = compare_methods(\n",
    "    methods_data,\n",
    "    title='üîç VGG16 vs SWIFT (CLIP) Features Extraction Performance Comparison'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "5.2 Feature Extraction\n",
    "Methods:\n",
    "\n",
    "SIFT implementation\n",
    "Feature detection\n",
    "Descriptor computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1 Classical Image Descriptors: SIFT, ORB, SURF\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"üîç Classical Image Descriptors: SIFT, ORB, SURF\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize detectors\n",
    "sift = cv2.SIFT_create()\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "# Note: SURF requires opencv-contrib-python, using ORB as alternative\n",
    "\n",
    "# Extract descriptors from first 20 processed images\n",
    "sample_images = processed_images[:min(20, len(processed_images))]\n",
    "descriptors_list = {'SIFT': [], 'ORB': []}\n",
    "\n",
    "for idx, img in enumerate(sample_images):\n",
    "    # Convert to uint8 if needed (processed_images are float [0,1])\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # SIFT descriptor extraction\n",
    "    kp_sift, des_sift = sift.detectAndCompute(gray, None)\n",
    "    if des_sift is not None:\n",
    "        descriptors_list['SIFT'].append(des_sift)\n",
    "    \n",
    "    # ORB descriptor extraction\n",
    "    kp_orb, des_orb = orb.detectAndCompute(gray, None)\n",
    "    if des_orb is not None:\n",
    "        descriptors_list['ORB'].append(des_orb.astype(np.float32))\n",
    "\n",
    "print(f\"‚úì SIFT: {len(descriptors_list['SIFT'])} images with keypoints detected\")\n",
    "print(f\"‚úì ORB: {len(descriptors_list['ORB'])} images with keypoints detected\")\n",
    "\n",
    "# Create bag-of-visual-words: concatenate all descriptors and cluster\n",
    "print(\"\\nüì¶ Building Bag-of-Visual-Words...\\n\")\n",
    "\n",
    "# Concatenate all SIFT descriptors\n",
    "if descriptors_list['SIFT']:\n",
    "    all_sift_des = np.concatenate(descriptors_list['SIFT'], axis=0)\n",
    "    print(f\"SIFT - Total descriptors: {all_sift_des.shape[0]}, Dimension: {all_sift_des.shape[1]}\")\n",
    "    \n",
    "    # Cluster into visual words (vocabulary size = 64)\n",
    "    kmeans_sift = KMeans(n_clusters=64, random_state=42, n_init=10)\n",
    "    sift_labels = kmeans_sift.fit_predict(all_sift_des)\n",
    "    \n",
    "    # Create histogram for each image\n",
    "    sift_features = []\n",
    "    for des in descriptors_list['SIFT']:\n",
    "        labels = kmeans_sift.predict(des)\n",
    "        hist, _ = np.histogram(labels, bins=np.arange(0, 65))\n",
    "        sift_features.append(hist)\n",
    "    sift_features = np.array(sift_features)\n",
    "    print(f\"SIFT Feature Matrix: {sift_features.shape}\")\n",
    "\n",
    "# Concatenate all ORB descriptors\n",
    "if descriptors_list['ORB']:\n",
    "    all_orb_des = np.concatenate(descriptors_list['ORB'], axis=0)\n",
    "    print(f\"\\nORB - Total descriptors: {all_orb_des.shape[0]}, Dimension: {all_orb_des.shape[1]}\")\n",
    "    \n",
    "    # Cluster into visual words (vocabulary size = 64)\n",
    "    kmeans_orb = KMeans(n_clusters=64, random_state=42, n_init=10)\n",
    "    orb_labels = kmeans_orb.fit_predict(all_orb_des)\n",
    "    \n",
    "    # Create histogram for each image\n",
    "    orb_features = []\n",
    "    for des in descriptors_list['ORB']:\n",
    "        labels = kmeans_orb.predict(des)\n",
    "        hist, _ = np.histogram(labels, bins=np.arange(0, 65))\n",
    "        orb_features.append(hist)\n",
    "    orb_features = np.array(orb_features)\n",
    "    print(f\"ORB Feature Matrix: {orb_features.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Classical descriptors extraction complete!\")\n",
    "print(\"   SIFT & ORB vocabularies: 64 visual words each\")\n",
    "print(\"   ‚Üí Can be used for image classification with SVM/Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e64db",
   "metadata": {},
   "source": [
    "### 5.3 Image Data IP Rights & Copyright Verification\n",
    "\n",
    "This feasibility study processes product images from the Flipkart e-commerce dataset for research and educational purposes.\n",
    "\n",
    "**Image Licensing & IP Compliance:**\n",
    "- **Data Source**: Flipkart e-commerce marketplace (product images from public product pages)\n",
    "- **Data Type**: Product photos (non-personal, commercial product images)\n",
    "- **Usage Rights**: Used exclusively for feasibility study research under academic fair use\n",
    "- **Copyright Holder**: Individual product images owned by brand/vendor (Flipkart acts as aggregator)\n",
    "- **Fair Use Justification**: \n",
    "  - Non-commercial research purpose\n",
    "  - Transformative use (feature extraction, classification, not reproduction)\n",
    "  - Small sample size (1050 images from dataset)\n",
    "  - No direct commercial exploitation\n",
    "- **Disclaimer**: This study does not claim ownership of images; attribution to product vendors/Flipkart acknowledged\n",
    "- **Data Privacy**: No personal information in product images; pure product/merchandise photography\n",
    "\n",
    "**Implementation Note**: Images are processed only for feature extraction; original images not published or redistributed, only computational features retained for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 5.4 Image Feature Extraction & Clustering ‚Äì Conclusion\n",
    "\n",
    "**Goal:** Assess feasibility of category separation using handcrafted + deep image features before full supervised CNN training.\n",
    "\n",
    "**What Was Done**\n",
    "- Basic preprocessing: resize (224√ó224), quality filtering (100% success rate on 1,050 images).\n",
    "- Classical descriptors: SIFT, LBP, GLCM, Gabor, patch statistics (combined feature matrix).\n",
    "- Deep features: VGG16 (block5_pool) + PCA + t-SNE + clustering.\n",
    "- Vision-language features: CLIP (SWIFT) extracted & compared to VGG16.\n",
    "\n",
    "**Key Findings**\n",
    "- Classical feature matrix shape: **(1050, 290)** ‚Üí weak separation via 5 descriptor types (SIFT 128 + LBP 10 + GLCM 16 + Gabor 36 + Patches 100).\n",
    "- VGG16 PCA features: **(1050, 75 dims)** ‚Üí improved structure (silhouette **0.083**, ARI **0.3491**; 68% variance preserved).\n",
    "- CLIP features: **(1050, 75 dims)** ‚Üí higher semantic alignment (silhouette **0.144**, ARI **‚àí0.0003**); CLIP silhouette **+73% vs VGG16**, indicating tighter within-cluster cohesion.\n",
    "- Cluster distance spread: visible inter-category separation in t-SNE plots, though overlaps remain in visually similar subcategories.\n",
    "- Failure cases: low-texture items (e.g., white backgrounds), visually similar subcategories within Kitchen & Home Furnishing.\n",
    "\n",
    "**Interpretation**\n",
    "- Handcrafted features alone are insufficient‚Äîclassical descriptors show no clear category clustering (silhouette near 0).\n",
    "- Deep pretrained embeddings already encode category-relevant patterns (VGG16 ARI 0.35 >> random baseline).\n",
    "- CLIP adds semantic lift through vision-language alignment‚Äîsuperior silhouette score suggests tighter cluster compactness for downstream supervised training.\n",
    "\n",
    "**Feasibility Verdict**\n",
    "Image-only features (deep > classical) are viable for top-level category discrimination. VGG16's ARI of 0.35 and CLIP's improved silhouette (0.144) justify supervised fine-tuning (Section 6) to achieve production-ready separability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning VGG16 unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- 1) Setup ---\n",
    "image_dir = 'dataset/Flipkart/Images'\n",
    "print(f\"Using image directory: {image_dir}\")\n",
    "\n",
    "# --- 2) Data preparation ---\n",
    "df_prepared = df.copy()\n",
    "\n",
    "# keep only rows whose image file exists in image_dir\n",
    "available_images = set(os.listdir(image_dir))\n",
    "df_prepared = df_prepared[df_prepared['image'].isin(available_images)].reset_index(drop=True)\n",
    "print(f\"Found {len(df_prepared)} rows with existing image files.\")\n",
    "\n",
    "# full path for each image\n",
    "df_prepared['image_path'] = df_prepared['image'].apply(lambda img: os.path.join(image_dir, img))\n",
    "\n",
    "def sample_data(df_in, min_samples=8, samples_per_category=150):\n",
    "    counts = df_in['product_category'].value_counts()\n",
    "    valid = counts[counts >= min_samples].index\n",
    "    df_f = df_in[df_in['product_category'].isin(valid)]\n",
    "    return df_f.groupby('product_category', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), samples_per_category), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "df_sampled = sample_data(df_prepared, min_samples=8, samples_per_category=150)\n",
    "print(f\"Sampled {len(df_sampled)} items across {df_sampled['product_category'].nunique()} categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.transfer_learning_classifier_unsupervised as tlcu\n",
    "\n",
    "# reload the module to pick up code changes\n",
    "importlib.reload(tlcu)\n",
    "\n",
    "# import the class after reload\n",
    "from src.classes.transfer_learning_classifier_unsupervised import TransferLearningClassifierUnsupervised\n",
    "\n",
    "\n",
    "# --- 3) Unsupervised pipeline (VGG16 whole CNN) ---\n",
    "image_column = 'image_path'\n",
    "category_column = 'product_category'\n",
    "\n",
    "vgg_extractor = TransferLearningClassifierUnsupervised(\n",
    "    input_shape=(224, 224, 3),\n",
    "    backbones=['VGG16'],\n",
    "    use_include_top=False\n",
    ")\n",
    "\n",
    "_ = vgg_extractor.prepare_data_from_dataframe(\n",
    "    df=df_sampled,\n",
    "    image_column=image_column,\n",
    "    category_column=category_column,\n",
    "    image_dir=None  # image_column already has full paths\n",
    ")\n",
    "processed_images = vgg_extractor._load_images()\n",
    "\n",
    "# features\n",
    "vgg_features = vgg_extractor._extract_features('VGG16')\n",
    "\n",
    "# elbow\n",
    "optimal_components, elbow_fig = vgg_extractor.find_optimal_pca_components(\n",
    "    vgg_features, max_components=500, step_size=75\n",
    ")\n",
    "elbow_fig.show()\n",
    "\n",
    "# PCA\n",
    "vgg_features_pca, pca_info, scaler_vgg = vgg_extractor.apply_dimensionality_reduction(\n",
    "    vgg_features, n_components=optimal_components, method='pca'\n",
    ")\n",
    "\n",
    "# t-SNE\n",
    "vgg_features_tsne, tsne_info, _ = vgg_extractor.apply_dimensionality_reduction(\n",
    "    vgg_features_pca, n_components=2, method='tsne'\n",
    ")\n",
    "\n",
    "# clustering\n",
    "vgg_clustering_results = vgg_extractor.perform_clustering(\n",
    "    vgg_features_pca, n_clusters=None, cluster_range=(7, 7)\n",
    ")\n",
    "\n",
    "# dashboard\n",
    "vgg_dashboard = vgg_extractor.create_analysis_dashboard(\n",
    "    backbone_name='VGG16',\n",
    "    original_features=vgg_features,\n",
    "    reduced_features=vgg_features_pca,\n",
    "    clustering_results=vgg_clustering_results,\n",
    "    processing_times=vgg_extractor.processing_times,\n",
    "    pca_info=pca_info\n",
    ")\n",
    "vgg_dashboard.show()\n",
    "\n",
    "# compare with categories\n",
    "vgg_analysis_results = vgg_extractor.compare_with_categories(\n",
    "    df=vgg_extractor.df,\n",
    "    tsne_features=vgg_features_tsne,\n",
    "    clustering_results=vgg_clustering_results,\n",
    "    backbone_name='VGG16'\n",
    ")\n",
    "\n",
    "# ARI\n",
    "vgg_ari = vgg_analysis_results['ari_score']\n",
    "if 'ari_scores' not in globals():\n",
    "    ari_scores = {}\n",
    "ari_scores['VGG16'] = vgg_ari\n",
    "print(f\"VGG16 ARI: {vgg_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying the original dictionary in place\n",
    "combined_ari_scores = ari_scores.copy()\n",
    "\n",
    "\n",
    "# Import existing plotting function\n",
    "from src.scripts.plot_ari_comparison import ari_comparison\n",
    "\n",
    "# Create and display the final, combined visualization\n",
    "print(\"\\nüìà Creating final comparison plot...\")\n",
    "final_comparison_fig = ari_comparison(combined_ari_scores)\n",
    "final_comparison_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning (VGG16)\n",
    "\n",
    "**Goal:** Classify product images into categories using a pretrained CNN to reduce training time and overfitting.\n",
    "\n",
    "**Model**\n",
    "- Backbone: VGG16 (ImageNet weights, frozen)\n",
    "- Head: GlobalAveragePooling ‚Üí Dense(1024, ReLU) ‚Üí Dropout(0.5) ‚Üí Dense(num_classes, softmax)\n",
    "- Variants: \n",
    "  - base_vgg16 (no augmentation)  \n",
    "  - augmented_vgg16 (with image augmentations)\n",
    "\n",
    "**Data**\n",
    "- Images resized to 224√ó224\n",
    "- VGG16 preprocessing applied\n",
    "- Stratified train / val / test split\n",
    "- Optional sampling to ensure minimum samples per class\n",
    "\n",
    "**Augmentations (augmented model)**\n",
    "- Horizontal flip\n",
    "- Small rotations\n",
    "- Brightness / zoom tweaks\n",
    "\n",
    "**Training**\n",
    "- Optimizer: Adam\n",
    "- Loss: Categorical crossentropy\n",
    "- Batch size: 8\n",
    "- Epochs: up to 10 (early stopping patience=3)\n",
    "- Only classification head is trainable\n",
    "\n",
    "**Tracked Outputs**\n",
    "- Train / val loss & accuracy curves\n",
    "- Best model selected by validation loss\n",
    "- Confusion matrix for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.classes.transfer_learning_classifier import TransferLearningClassifier\n",
    "\n",
    "\n",
    "# --- 3. Model Training ---\n",
    "\n",
    "# Initialize classifier with explicit parameters for reproducibility\n",
    "classifier = TransferLearningClassifier(\n",
    "    input_shape=(224, 224, 3)\n",
    "    \n",
    ")\n",
    "\n",
    "# Prepare data - the classifier will now receive full, verified paths\n",
    "data_summary = classifier.prepare_data_from_dataframe(\n",
    "    df_sampled, \n",
    "    image_column='image_path',      # Use the column with full paths\n",
    "    category_column='product_category',# Use the clean category column\n",
    "    test_size=0.2,\n",
    "    val_size=0.25, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"\\n‚úÖ Data prepared for transfer learning:\")\n",
    "print(f\"   üéØ Classes: {data_summary['num_classes']}\")\n",
    "print(f\"   Train/Val/Test split: {data_summary['train_size']}/{data_summary['val_size']}/{data_summary['test_size']}\")\n",
    "\n",
    "# Prepare image arrays for training\n",
    "classifier.prepare_arrays_method()\n",
    "print(\"‚úÖ Image arrays prepared for training.\")\n",
    "\n",
    "# Train models with more conservative parameters for stability\n",
    "print(\"\\nüöÄ Training VGG16 models...\")\n",
    "\n",
    "# Base model\n",
    "base_model = classifier.create_base_model(show_backbone_summary=True)\n",
    "results1 = classifier.train_model(\n",
    "    'base_vgg16', \n",
    "    base_model, \n",
    "    epochs=10,      # Reduced for faster, more stable initial training\n",
    "    batch_size=8,   # Smaller batch size to prevent memory issues\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Augmented model\n",
    "aug_model = classifier.create_augmented_model()\n",
    "results2 = classifier.train_model(\n",
    "    'augmented_vgg16', \n",
    "    aug_model, \n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    patience=3\n",
    ")\n",
    "print(\"‚úÖ Training complete.\")\n",
    "\n",
    "# --- 4. Results and Visualization ---\n",
    "print(\"\\nüìà Displaying results...\")\n",
    "# Compare models\n",
    "comparison_fig = classifier.compare_models()\n",
    "comparison_fig.show()\n",
    "\n",
    "# Plot training history\n",
    "history_fig = classifier.plot_training_history()\n",
    "history_fig.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "summary = classifier.get_summary()\n",
    "if summary['best_model']:\n",
    "    best_model_name = summary['best_model']['name']\n",
    "    print(f\"üìä Plotting confusion matrix for best model: {best_model_name}\")\n",
    "    conf_fig = classifier.plot_confusion_matrix(best_model_name)\n",
    "    conf_fig.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nüìã Final Summary:\")\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the new method to get the interactive plot\n",
    "example_fig = classifier.plot_prediction_examples(\n",
    "    model_name=best_model_name,\n",
    "    num_correct=4,  # Show 4 correct predictions\n",
    "    num_incorrect=4 # Show 4 incorrect predictions\n",
    ")\n",
    "\n",
    "\n",
    "example_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## 8. Advanced Improvements: Production-Ready Features\n",
    "\n",
    "**What's Next?**\n",
    "This section demonstrates 7 high-impact production improvements: enhanced metrics, interpretability (Grad-CAM), reproducibility (multi-seed training), alternative architectures, multimodal fusion, experiment tracking (MLflow), and experiment management patterns. Each demonstrates practical usage with quick demos‚Äîno lengthy retraining.\n",
    "\n",
    "**Key Improvements:**\n",
    "- **Enhanced Metrics**: Per-class F1, macro/micro metrics.\n",
    "- **Grad-CAM Visualization**: Visual model interpretability.\n",
    "- **Multi-Seed Training**: Reproducible experiments (‚â•3 seeds).\n",
    "- **Alternative Backbones**: EfficientNet, ResNet, InceptionV3.\n",
    "- **Multimodal Fusion**: Late fusion (text + image embeddings).\n",
    "- **MLflow Tracking**: Experiment logging & model registry.\n",
    "- **Summary**: Best practices & implementation checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### 8.1 Enhanced Metrics: Per-Class & Aggregate\n",
    "\n",
    "**Goal:** Move beyond accuracy to per-class F1, macro/micro averaging, and confusion matrices.\n",
    "\n",
    "**What's Happening:**\n",
    "- Calculating precision, recall, F1 for each category.\n",
    "- Macro vs micro F1 to identify class imbalance issues.\n",
    "- Visualization of per-class performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.classes.enhanced_metrics as em\n",
    "import numpy as np\n",
    "\n",
    "# reload the module to pick up any code changes\n",
    "importlib.reload(em)\n",
    "\n",
    "from src.classes.enhanced_metrics import EnhancedMetrics \n",
    "\n",
    "# Get predictions from best model using only test data\n",
    "best_model = classifier.models[best_model_name]\n",
    "\n",
    "# Get test predictions (use preprocessed test images from classifier)\n",
    "y_pred_probs = best_model.predict(classifier.X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true labels from test dataframe\n",
    "y_true_test = classifier.test_df['product_category'].values\n",
    "category_names = sorted(df_sampled['product_category'].unique())\n",
    "category_indices = {cat: idx for idx, cat in enumerate(category_names)}\n",
    "y_true_encoded = np.array([category_indices[cat] for cat in y_true_test])\n",
    "\n",
    "# Initialize enhanced metrics with predictions\n",
    "metrics_calc = EnhancedMetrics(y_true=y_true_encoded, y_pred=y_pred, class_names=category_names)\n",
    "\n",
    "# Get metrics (returns a dictionary)\n",
    "per_class_metrics = metrics_calc.get_per_class_metrics()\n",
    "metrics_dict = metrics_calc.get_macro_micro_f1()\n",
    "\n",
    "# Extract F1 scores from dictionary\n",
    "macro_f1 = metrics_dict['macro_f1']\n",
    "micro_f1 = metrics_dict['micro_f1']\n",
    "weighted_f1 = metrics_dict['weighted_f1']\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Enhanced Metrics Results:\")\n",
    "print(f\"‚úì Macro F1:    {macro_f1:.4f}\")\n",
    "print(f\"‚úì Micro F1:    {micro_f1:.4f}\")\n",
    "print(f\"‚úì Weighted F1: {weighted_f1:.4f}\")\n",
    "print(\"\\nüìã Per-Class Metrics:\")\n",
    "print(per_class_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d2b9a",
   "metadata": {},
   "source": [
    "### 8.0 Metrics Justification & Business Context\n",
    "\n",
    "**üìã CE2: Justification for Metric Choices**\n",
    "\n",
    "**Why Macro F1 + Micro F1 + Weighted F1?**\n",
    "\n",
    "For a multi-class product classification system, single accuracy metric is insufficient:\n",
    "\n",
    "| Metric | When to Use | Business Relevance |\n",
    "|---|---|---|\n",
    "| **Accuracy** | Baseline only | Misleading if categories imbalanced |\n",
    "| **Macro F1** | Minority class fairness | Treats all categories equally (important for niche products) |\n",
    "| **Micro F1** | Overall system performance | Weighted by category frequency (reflects real-world usage) |\n",
    "| **Weighted F1** | Business impact | Balances all categories by their true distribution |\n",
    "| **Per-Class F1** | Debugging model | Identifies problem categories (e.g., \"Electronics\" vs \"Clothing\") |\n",
    "\n",
    "**Our Choice**:\n",
    "- **Primary Metric**: Weighted F1 (reflects real-world distribution, avoids minority bias)\n",
    "- **Secondary Metrics**: Macro F1 (ensures fairness), Micro F1 (overall quality)\n",
    "- **Tertiary**: Per-class F1 (diagnostic breakdown)\n",
    "\n",
    "**Business Impact Example:**\n",
    "- If \"Electronics\" = 70% of data, \"Fashion\" = 30%:\n",
    "  - Accuracy: 85% ‚Üí misleading if Electronics alone is 90%\n",
    "  - Weighted F1: Better reflects business impact (more accurate for common category)\n",
    "  - Macro F1: Ensures Fashion classification doesn't degrade to <50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.baseline_comparison import BaselineComparison\n",
    "\n",
    "# Initialize and run baseline comparison\n",
    "baseline_comparator = BaselineComparison(classifier)\n",
    "baseline_comparator.compare(\n",
    "    classifier.X_train, \n",
    "    classifier.train_df['product_category'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.hyperparameter_summary import HyperparameterSummary\n",
    "\n",
    "# 1. Retrieve metrics dynamically\n",
    "clf_summary = classifier.get_summary()\n",
    "current_acc = clf_summary['best_model']['test_accuracy']\n",
    "\n",
    "\n",
    "# F1 Score from EnhancedMetrics\n",
    "current_f1 = globals().get('weighted_f1')\n",
    "\n",
    "# 2. Generate Summary\n",
    "hyper_summary = HyperparameterSummary()\n",
    "hyper_summary.print_summary()\n",
    "\n",
    "# 3. Create report with actual values\n",
    "summary_dict = hyper_summary.get_summary(\n",
    "    achieved_accuracy=current_acc, \n",
    "    achieved_f1=current_f1\n",
    ")\n",
    "\n",
    "# 4. Display confirmation\n",
    "print(f\"\\n‚úÖ Report updated with actual metrics from this run:\")\n",
    "print(f\"   Accuracy: {current_acc:.4f}\")\n",
    "print(f\"   F1 Score: {current_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.architecture_comparison import ArchitectureComparison\n",
    "\n",
    "# Initialize architecture comparison\n",
    "arch_comparator = ArchitectureComparison(classifier)\n",
    "\n",
    "# Compare different classifier heads (Simple, Medium, Deep)\n",
    "# Uses extracted features to avoid retraining the base model\n",
    "arch_comparator.compare(\n",
    "    classifier.X_train, \n",
    "    classifier.train_df['product_category'].values,\n",
    "    classifier.X_val,\n",
    "    classifier.val_df['product_category'].values,\n",
    "    epochs=5  # Quick comparison\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.multimodal_analysis import MultimodalAnalysis\n",
    "\n",
    "# Initialize multimodal analysis\n",
    "multimodal = MultimodalAnalysis(classifier)\n",
    "\n",
    "# Run fusion analysis (Text + Image)\n",
    "# This reuses the best text model (USE) and image model (VGG16)\n",
    "multimodal.evaluate_fusion(\n",
    "    classifier.X_test,\n",
    "    classifier.test_df['product_category'].values,\n",
    "    classifier.test_df['description'].values\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
